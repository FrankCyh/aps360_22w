{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhtOdxzd1ppr"
   },
   "source": [
    "# Lab 4: Data Imputation using an Autoencoder\n",
    "\n",
    "**Deadline**: Mon, Mar 07, 5:00pm\n",
    "\n",
    "**Late Penalty**:  Any work that is submitted between 0 hour and 24 hours past the deadline will receive a 20% grade deduction. No other late work is accepted. Quercus submission time will be used, not your local computer time. You can submit your labs as many times as you want before the deadline, so please submit often and early.\n",
    "\n",
    "**TA**: Shiva Akbari <shiva.akbari@mail.utoronto.ca>\n",
    "\n",
    "In this lab, you will build and train an autoencoder to impute (or \"fill in\") missing data. \n",
    "\n",
    "We will be using the\n",
    "Adult Data Set provided by the UCI Machine Learning Repository [1], available \n",
    "at https://archive.ics.uci.edu/ml/datasets/adult.\n",
    "The data set contains census record files of adults, including their\n",
    "age, martial status, the type of work they do, and other features. \n",
    "\n",
    "Normally, people use this data set to build a supervised classification\n",
    "model to classify whether a person is a high income earner.\n",
    "We will not use the dataset for this original intended purpose.\n",
    "\n",
    "Instead, we will perform the task of imputing (or \"filling in\") missing values in the dataset. For example,\n",
    "we may be missing one person's martial status, and another person's age, and\n",
    "a third person's level of education. Our model will predict the missing features \n",
    "based on the information that we do have about each person.\n",
    "\n",
    "We will use a variation of a denoising autoencoder to solve this data imputation\n",
    "problem. Our autoencoder will be trained using inputs that have one categorical feature artificially\n",
    "removed, and the goal of the autoencoder is to correctly reconstruct all features,\n",
    "including the one removed from the input.\n",
    "\n",
    "In the process, you are expected to learn to:\n",
    "\n",
    "1. Clean and process continuous and categorical data for machine learning.\n",
    "2. Implement an autoencoder that takes continuous and categorical (one-hot) inputs.\n",
    "3. Tune the hyperparameters of an autoencoder.\n",
    "4. Use baseline models to help interpret model performance.\n",
    "\n",
    "[1] Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "\n",
    "### What to submit\n",
    "\n",
    "Submit a PDF file containing all your code, outputs, and write-up. You can produce a PDF of your Google Colab file by going to File > Print and then save as PDF. The Colab instructions have more information (.html files are also acceptable).\n",
    "\n",
    "Do not submit any other files produced by your code.\n",
    "\n",
    "Include a link to your colab file in your submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbnrp2ig1pps"
   },
   "source": [
    "## Colab Link\n",
    "\n",
    "Include a link to your Colab file here. If you would like the TA to look at your\n",
    "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
    "file is publicly accessible at the time of submission**.\n",
    "\n",
    "Colab Link: https://colab.research.google.com/drive/11OSIdxpmHhPn2sr0r2dZsQ2xTEU5MrBw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z3p8N43E1ppt",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ROwtHcz1ppx"
   },
   "source": [
    "## Part 0\n",
    "\n",
    "We will be using a package called `pandas` for this assignment. \n",
    "\n",
    "If you are using Colab, `pandas` should already be available.\n",
    "If you are using your own computer,\n",
    "installation instructions for `pandas` are available here: \n",
    "https://pandas.pydata.org/pandas-docs/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IXQ7BP151ppz",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqXihb4Q1pp2"
   },
   "source": [
    "# Part 1. Data Cleaning [15 pt]\n",
    "\n",
    "The adult.data file is available at `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data`\n",
    "\n",
    "The function `pd.read_csv` loads the adult.data file into a pandas dataframe.\n",
    "You can read about the pandas documentation for `pd.read_csv` at\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EOMItFKn1pp3",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8d347ff3-a1a3-4b21-c73f-a5c86b08e7d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank.c/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "header = ['age', 'work', 'fnlwgt', 'edu', 'yredu', 'marriage', 'occupation',\n",
    " 'relationship', 'race', 'sex', 'capgain', 'caploss', 'workhr', 'country']\n",
    "df = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    names=header,\n",
    "    index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "62Ot405q1pp5",
    "outputId": "c90e1be4-182d-4816-c20f-5d65fe414844",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # 32561 rows, and 14 columns (attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr7YG-QY1pp8"
   },
   "source": [
    "### Part (a) Continuous Features [3 pt]\n",
    "\n",
    "For each of the columns `[\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]`, report the minimum, maximum, and average value across the dataset. \n",
    "\n",
    "Then, normalize each of the features `[\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]`\n",
    "so that their values are always between 0 and 1.\n",
    "Make sure that you are actually modifying the dataframe `df`. \n",
    "\n",
    "Like numpy arrays and torch tensors, \n",
    "pandas data frames can be sliced. For example, we can\n",
    "display the first 3 rows of the data frame (3 records) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9evSLsSa1pp9",
    "outputId": "cd1c2aee-df56-4df1-df16-3247d929a7b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>work</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>edu</th>\n",
       "      <th>yredu</th>\n",
       "      <th>marriage</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age               work  fnlwgt         edu  yredu             marriage  \\\n",
       "0   39          State-gov   77516   Bachelors     13        Never-married   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors     13   Married-civ-spouse   \n",
       "2   38            Private  215646     HS-grad      9             Divorced   \n",
       "\n",
       "           occupation    relationship    race    sex  capgain  caploss  \\\n",
       "0        Adm-clerical   Not-in-family   White   Male     2174        0   \n",
       "1     Exec-managerial         Husband   White   Male        0        0   \n",
       "2   Handlers-cleaners   Not-in-family   White   Male        0        0   \n",
       "\n",
       "   workhr         country  \n",
       "0      40   United-States  \n",
       "1      13   United-States  \n",
       "2      40   United-States  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3] # show the first 3 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOojI6W1pqA"
   },
   "source": [
    "Alternatively, we can slice based on column names, \n",
    "for example `df[\"race\"]`, `df[\"hr\"]`, or even index multiple columns \n",
    "like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4v6pp73A1pqB",
    "outputId": "80c34c3c-4df7-414d-ffe4-9ff73b81b68a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>yredu</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  yredu  capgain  caploss  workhr\n",
       "0   39     13     2174        0      40\n",
       "1   50     13        0        0      13\n",
       "2   38      9        0        0      40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf = df[[\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]] #/ choose specific columns\n",
    "subdf[:3] # show the first 3 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Nru2P0E1pqD"
   },
   "source": [
    "Numpy works nicely with pandas, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JXrS6tjp1pqE",
    "outputId": "29ee3639-30ae-4533-cb73-7cddf4be82a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2842700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(subdf[\"caploss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv5mbxDM1pqH"
   },
   "source": [
    "Just like numpy arrays, you can modify\n",
    "entire columns of data rather than one scalar element at a time.\n",
    "For example, the code  \n",
    "\n",
    "`df[\"age\"] = df[\"age\"] + 1` \n",
    "\n",
    "would increment everyone's age by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "k5rlWD7-1pqH",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "edaf66ac-58ce-4621-8453-8abc8f7e3536"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>yredu</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age         yredu       capgain       caploss        workhr\n",
       "count  32561.000000  32561.000000  32561.000000  32561.000000  32561.000000\n",
       "mean      38.581647     10.080679   1077.648844     87.303830     40.437456\n",
       "std       13.640433      2.572720   7385.292085    402.960219     12.347429\n",
       "min       17.000000      1.000000      0.000000      0.000000      1.000000\n",
       "25%       28.000000      9.000000      0.000000      0.000000     40.000000\n",
       "50%       37.000000     10.000000      0.000000      0.000000     40.000000\n",
       "75%       48.000000     12.000000      0.000000      0.000000     45.000000\n",
       "max       90.000000     16.000000  99999.000000   4356.000000     99.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: Report the minimum, maximum, and average value across the dataset.'''\n",
    "\n",
    "# Before normalization\n",
    "subdf.describe(percentiles=[.25, .5, .75]) #/ get some statistic of the data before the manipulation\n",
    "# Param {percentiles} : default percentile = [.25, .5, .75] \n",
    "# Param {exclude} {include} {datetime_is_numeric}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5wovWyO5ubbW",
    "outputId": "19fe4c3e-53dd-4f10-8264-ef1115ce2dc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>yredu</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.295639</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>0.605379</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.020042</td>\n",
       "      <td>0.402423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.186855</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>0.171515</td>\n",
       "      <td>0.073854</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.125994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.273973</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.424658</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt         yredu       capgain       caploss  \\\n",
       "count  32561.000000  3.256100e+04  32561.000000  32561.000000  32561.000000   \n",
       "mean       0.295639  1.897784e+05      0.605379      0.010777      0.020042   \n",
       "std        0.186855  1.055500e+05      0.171515      0.073854      0.092507   \n",
       "min        0.000000  1.228500e+04      0.000000      0.000000      0.000000   \n",
       "25%        0.150685  1.178270e+05      0.533333      0.000000      0.000000   \n",
       "50%        0.273973  1.783560e+05      0.600000      0.000000      0.000000   \n",
       "75%        0.424658  2.370510e+05      0.733333      0.000000      0.000000   \n",
       "max        1.000000  1.484705e+06      1.000000      1.000000      1.000000   \n",
       "\n",
       "             workhr  \n",
       "count  32561.000000  \n",
       "mean       0.402423  \n",
       "std        0.125994  \n",
       "min        0.000000  \n",
       "25%        0.397959  \n",
       "50%        0.397959  \n",
       "75%        0.448980  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: normalize each of the features [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"] so that their values are always between 0 and 1 '''\n",
    "col_to_nomalize = [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]\n",
    "\n",
    "#/ How to normalize: (val - min) / (max - min)\n",
    "\n",
    "for i in range(len(col_to_nomalize)):\n",
    "    col = df[col_to_nomalize[i]] #/ col is a vector\n",
    "    col_range = col.max() - col.min()\n",
    "    df[col_to_nomalize[i]] = (col - col.min()) / col_range\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbfMly4R1pqK"
   },
   "source": [
    "### Part (b) Categorical Features [1 pt]\n",
    "\n",
    "What percentage of people in our data set are male? Note that the data labels all have an unfortunate space in the beginning, e.g. \" Male\" instead of \"Male\".\n",
    "\n",
    "What percentage of people in our data set are female?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DjAjcsB_1pqK",
    "outputId": "cd7201c3-007c-4fea-d955-4ce34ff808b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of male: 66.9\n",
      "Percentage of female: 33.1\n"
     ]
    }
   ],
   "source": [
    "''' Answer: percentage of male and female'''\n",
    "# hint: you can do something like this in pandas\n",
    "num_male = sum(df[\"sex\"] == \" Male\")\n",
    "num_female = sum(df[\"sex\"] == \" Female\")\n",
    "\n",
    "print(\"Percentage of male: {}\".format(round(100* num_male/len(df[\"sex\"]),1)))\n",
    "print(\"Percentage of female: {}\".format(round(100* num_female/len(df[\"sex\"]),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGVw7pqL1pqN"
   },
   "source": [
    "### Part (c) [2 pt]\n",
    "\n",
    "Before proceeding, we will modify our data frame in a couple more ways:\n",
    "\n",
    "1. We will restrict ourselves to using a subset of the features (to simplify our autoencoder)\n",
    "2. We will remove any records (rows) already containing missing values, and store them in a second dataframe. We will only use records without missing values to train our autoencoder.\n",
    "\n",
    "Both of these steps are done for you, below.\n",
    "\n",
    "How many records contained missing features? What percentage of records were removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z6ewPUdv1pqO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contcols = [\"age\", \"yredu\", \"capgain\", \"caploss\", \"workhr\"]\n",
    "catcols = [\"work\", \"marriage\", \"occupation\", \"edu\", \"relationship\", \"sex\"] #/ attributes that need one-hot encoding\n",
    "features = contcols + catcols #/ extract feature with attributes from contcols and catcols\n",
    "df = df[features] #/ Extracted all attributes actually, filter out value with NaN in the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "fjdVll5a1pqQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0216f111-3e2b-41a0-b2d1-402394d07e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of record that contained missing features: 1843\n",
      "Percentage of record that contained missing features: 5.7\n"
     ]
    }
   ],
   "source": [
    "missing = pd.concat([df[c] == \" ?\" for c in catcols], axis=1).any(axis=1)\n",
    "df_with_missing = df[missing]\n",
    "df_not_missing = df[~missing]\n",
    "\n",
    "''' Answer: number and percentage of record that contained missing features'''\n",
    "print(\"Number of record that contained missing features: {}\".format(len(df_with_missing)))\n",
    "print(\"Percentage of record that contained missing features: {}\".format(round(100 * len(df_with_missing) / df.shape[0], 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuEpndTQ1pqU"
   },
   "source": [
    "### Part (d) One-Hot Encoding [1 pt]\n",
    "\n",
    "What are all the possible values of the feature \"work\" in `df_not_missing`? You may find the Python function `set` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iKFh4owE1pqV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4d03dd11-dc65-4b97-c72c-fdee930b619b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Private             22696\n",
       " Self-emp-not-inc     2541\n",
       " Local-gov            2093\n",
       " State-gov            1298\n",
       " Self-emp-inc         1116\n",
       " Federal-gov           960\n",
       " Without-pay            14\n",
       "Name: work, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: possible values for attribute 'work' '''\n",
    "df_not_missing['work'].value_counts()\n",
    "# df_not_missing['work'].unique() or set(df_not_missing[\"work\"]) provide only get the values, not the counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COv3HaKr1pqY"
   },
   "source": [
    "We will be using a one-hot encoding to represent each of the categorical variables.\n",
    "Our autoencoder will be trained using these one-hot encodings.\n",
    "\n",
    "We will use the pandas function `get_dummies` to produce one-hot encodings\n",
    "for all of the categorical variables in `df_not_missing`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eKlSYmJg1pqZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(df_not_missing) #/ produce one-hot encodings for all of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JuOVC2FKubbZ",
    "outputId": "d80f2594-3ba5-4153-9e2f-e700fdfc6f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workhr\n",
      "work_ Federal-gov\n",
      "work_ Local-gov\n",
      "work_ Private\n",
      "work_ Self-emp-inc\n",
      "work_ Self-emp-not-inc\n",
      "work_ State-gov\n",
      "work_ Without-pay\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Print the one-hot encoding for work in data\n",
    "data_features = data.keys()\n",
    "data_features_with_work = [value for value in data_features if 'work' in value.lower()]\n",
    "for i in range(len(data_features_with_work)):\n",
    "    print(data_features_with_work[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3y7nTZ7H1pqb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f81b111a-04ce-470c-8b0b-323d6ee3ad14",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>yredu</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "      <th>work_ Federal-gov</th>\n",
       "      <th>work_ Local-gov</th>\n",
       "      <th>work_ Private</th>\n",
       "      <th>work_ Self-emp-inc</th>\n",
       "      <th>work_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>edu_ Prof-school</th>\n",
       "      <th>edu_ Some-college</th>\n",
       "      <th>relationship_ Husband</th>\n",
       "      <th>relationship_ Not-in-family</th>\n",
       "      <th>relationship_ Other-relative</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.02174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age     yredu  capgain  caploss    workhr  work_ Federal-gov  \\\n",
       "0  0.301370  0.800000  0.02174      0.0  0.397959                  0   \n",
       "1  0.452055  0.800000  0.00000      0.0  0.122449                  0   \n",
       "2  0.287671  0.533333  0.00000      0.0  0.397959                  0   \n",
       "\n",
       "   work_ Local-gov  work_ Private  work_ Self-emp-inc  work_ Self-emp-not-inc  \\\n",
       "0                0              0                   0                       0   \n",
       "1                0              0                   0                       1   \n",
       "2                0              1                   0                       0   \n",
       "\n",
       "   ...  edu_ Prof-school  edu_ Some-college  relationship_ Husband  \\\n",
       "0  ...                 0                  0                      0   \n",
       "1  ...                 0                  0                      1   \n",
       "2  ...                 0                  0                      0   \n",
       "\n",
       "   relationship_ Not-in-family  relationship_ Other-relative  \\\n",
       "0                            1                             0   \n",
       "1                            0                             0   \n",
       "2                            1                             0   \n",
       "\n",
       "   relationship_ Own-child  relationship_ Unmarried  relationship_ Wife  \\\n",
       "0                        0                        0                   0   \n",
       "1                        0                        0                   0   \n",
       "2                        0                        0                   0   \n",
       "\n",
       "   sex_ Female  sex_ Male  \n",
       "0            0          1  \n",
       "1            0          1  \n",
       "2            0          1  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwjDg1uM1pqe"
   },
   "source": [
    "### Part (e) One-Hot Encoding [2 pt]\n",
    "\n",
    "The dataframe `data` contains the cleaned and normalized data that we will use to train our denoising autoencoder.\n",
    "\n",
    "How many **columns** (features) are in the dataframe `data`?\n",
    "\n",
    "Briefly explain where that number come from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yjZ5N0Tl1pqf",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3186e258-da3a-4f77-d072-359a623b147d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns:  57\n"
     ]
    }
   ],
   "source": [
    "''' Answer: Number of columns in the dataframe data'''\n",
    "print('Number of columns: ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEJ0Ci3l1pqh"
   },
   "source": [
    "### Part (f) One-Hot Conversion [3 pt]\n",
    "\n",
    "We will convert the pandas data frame `data` into numpy, so that\n",
    "it can be further converted into a PyTorch tensor.\n",
    "However, in doing so, we lose the column label information that\n",
    "a panda data frame automatically stores.\n",
    "\n",
    "Complete the function `get_categorical_value` that will return\n",
    "the named value of a feature given a one-hot embedding.\n",
    "You may find the global variables `cat_index` and `cat_values`\n",
    "useful. (Display them and figure out what they are first.)\n",
    "\n",
    "We will need this function in the next part of the lab\n",
    "to interpret our autoencoder outputs. So, the input\n",
    "to our function `get_categorical_values` might not \n",
    "actually be \"one-hot\" -- the input may instead \n",
    "contain real-valued predictions from our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original types: \n",
      "<class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> <class 'numpy.float64'> "
     ]
    }
   ],
   "source": [
    "print(\"original types: \")\n",
    "for i in range(len(data.values[0])):\n",
    "    print(type(data.values[0][i]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZmovX6gu1pqi",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "datanp = data.values.astype(np.float32) #/ convert from numpy.float64 to numpy.float32\n",
    "print(type(datanp[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YRIa5MBd1pql",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cat_index = {}  # Mapping of feature -> start index of feature in a record\n",
    "cat_values = {} # Mapping of feature -> list of categorical values the feature can take\n",
    "\n",
    "# build up the cat_index and cat_values dictionary\n",
    "for i, header in enumerate(data.keys()): #/ attributes of data\n",
    "    if \"_\" in header: # categorical header\n",
    "        feature, value = header.split() #/ default value for delimeter is space \" \"\n",
    "        # Example: 'work_ Federal-gov' will be split into 'work_\" and \"Federal-gov\"\n",
    "        feature = feature[:-1] # remove the last char; it is always an underscore\n",
    "        # Example: 'work_' will be cut to 'work'\n",
    "        if feature not in cat_index:\n",
    "            cat_index[feature] = i\n",
    "            # Example: {'work': 5}\n",
    "            cat_values[feature] = [value]\n",
    "            # Example: {'work': ['Federal-gov']}\n",
    "        else:\n",
    "            cat_values[feature].append(value)\n",
    "            # Example: {'work': ['Federal-gov', 'Local-gov']}\n",
    "\n",
    "\n",
    "# Description: Return the portion of `record` that is the one-hot encoding of `feature`. For example, since the feature \"work\" is stored in the indices[5:12] in each record, calling `get_range(record, \"work\") is equivalent to accessing `record[5:12]`.\n",
    "def get_onehot(record, feature):\n",
    "    # Param1 {record} : a numpy array representing one record, formatted the same way as a row in `data.np\n",
    "    # Param2 {feature} : a string, should be an element of `catcols`\n",
    "\n",
    "    start_index = cat_index[feature]\n",
    "    stop_index = cat_index[feature] + len(cat_values[feature])\n",
    "    return record[start_index:stop_index]\n",
    "\n",
    "\n",
    "def get_categorical_value(onehot, feature):\n",
    "    # Description: Return the categorical value name of a feature given a one-hot vector representing the feature.\n",
    "    # Param1 {onehot} : a numpy array one-hot representation of the feature, only contains the attribute of this feature\n",
    "    # Param2 {feature} : a string, should be an element of `catcols`\n",
    "\n",
    "    # Examples:\n",
    "    # >>> get_categorical_value(np.array([0., 0., 0., 0., 0., 1., 0.]), \"work\"\n",
    "    # 'State-gov'\n",
    "    # >>> get_categorical_value(np.array([0.1, 0., 1.1, 0.2, 0., 1., 0.]), \"work\")\n",
    "    # 'Private'\n",
    "    \n",
    "    #/ only turn the maximum value to 1, and disregard the other ones, setting to 0\n",
    "    #/ The normalized input will comes from the last activation of training\n",
    "\n",
    "    ''' Answer: implementation of get_categorical_value'''\n",
    "    return cat_values[feature][np.argmax(onehot)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "T_XXxZdh1pqv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# more useful code, used during training, that depends on the function\n",
    "# you write above\n",
    "\n",
    "def get_feature(record, feature):\n",
    "    # Description: Return the categorical feature value of a record\n",
    "    onehot = get_onehot(record, feature)\n",
    "    return get_categorical_value(onehot, feature) #/ get the one-hot encoding and feed to get_categorical_value to get largest index\n",
    "\n",
    "def get_features(record):\n",
    "    # Description: Return a dictionary of all categorical feature values of a record\n",
    "    return { f: get_feature(record, f) for f in catcols }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_5ZZR_J1pqy"
   },
   "source": [
    "### Part (g) Train/Test Split [3 pt]\n",
    "\n",
    "Randomly split the data into approximately 70% training, 15% validation and 15% test.\n",
    "\n",
    "Report the number of items in your training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TE_fTJJf1pqz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "725e08ba-bcde-463f-e870-e0979e3b7fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of traing data: 21503\n",
      "Number of validation data: 4607\n",
      "Number of test data: 4608\n",
      "Number of rows in datanp: 30718\n"
     ]
    }
   ],
   "source": [
    "# set the numpy seed for reproducibility\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html\n",
    "np.random.seed(50)\n",
    "\n",
    "''' Answer: split data into approximately 70% training, 15% validation and 15% test'''\n",
    "\n",
    "divider1 = round(len(data) * 0.7)\n",
    "divider2 = round(len(data) * (0.7 + 0.15))\n",
    "train_data = datanp[: divider1]\n",
    "val_data = datanp[divider1 : divider2]\n",
    "test_data = datanp[divider2: ] # test_data = datanp[divider2: -1] will exclude the last one, incorrect\n",
    "\n",
    "''' Answer: Repor the number of your training, validation and test data'''\n",
    "print(\"Number of traing data: {}\".format(len(train_data)))\n",
    "print(\"Number of validation data: {}\".format(len(val_data)))\n",
    "print(\"Number of test data: {}\".format(len(test_data)))\n",
    "print(\"Number of rows in datanp: {}\".format(len(train_data) + len(val_data) + len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9wJAKOI1pq3"
   },
   "source": [
    "# Part 2. Model Setup [5 pt]\n",
    "\n",
    "### Part (a) [4 pt]\n",
    "\n",
    "Design a fully-connected autoencoder by modifying the `encoder` and `decoder`\n",
    "below.\n",
    "\n",
    "The input to this autoencoder will be the features of the `data`, with\n",
    "one categorical feature recorded as \"missing\". The output of the autoencoder\n",
    "should be the reconstruction of the same features, but with the missing\n",
    "value filled in.\n",
    "\n",
    "**Note**: Do not reduce the dimensionality of the input too much!\n",
    "The output of your embedding is expected to contain information \n",
    "about ~11 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "f3F--tdn1pq3",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.name = \"AutoEncoder\"\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        ''' Answer: Architecture of the model'''\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(57, 57),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(57, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 20)\n",
    "        )\n",
    "        #/ 3layers; embedding of size 20\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 57),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(57, 57),\n",
    "            nn.Sigmoid() # get to the range (0, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuEzTSAv1pq6"
   },
   "source": [
    "### Part (b) [1 pt]\n",
    "\n",
    "Explain why there is a sigmoid activation in the last step of the decoder.\n",
    "\n",
    "(**Note**: the values inside the data frame `data` and the training code in Part 3 might be helpful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HFLd0S4H1pq6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "72d80390-faca-4dd3-d759-10036196d449"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Answer: Why there is a sigmoid function '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: Why there is a sigmoid function '''\n",
    "# That is because our final layer from the model should be a one-hot encoding that will feed into the function get_categorical_value().\n",
    "# The function get_categorical_value(), as we defined above, receive normalized one-hot encoding from 0 to 1. Thus, we must normalize the value using a sigmoid function as the final activation function of the model.\n",
    "# We could use the max value between 0 and 1 to predict the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYwqFWVl1pq8"
   },
   "source": [
    "# Part 3. Training [18 pt] \n",
    "\n",
    "### Part (a) [6 pt]\n",
    "\n",
    "We will train our autoencoder in the following way:\n",
    "\n",
    "- In each iteration, we will hide one of the categorical features using the `zero_out_random_features` function\n",
    "- We will pass the data with one missing feature through the autoencoder, and obtain a reconstruction\n",
    "- We will check how close the reconstruction is compared to the original data -- including the value of the missing feature\n",
    "\n",
    "Complete the code to train the autoencoder, and plot the training and validation loss every few iterations.\n",
    "You may also want to plot training and validation \"accuracy\" every few iterations, as we will define in\n",
    "part (b). You may also want to checkpoint your model every few iterations or epochs.\n",
    "\n",
    "Use `nn.MSELoss()` as your loss function. (Side note: you might recognize that this loss function is not\n",
    "ideal for this problem, but we will use it anyway.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IDQA_-dS1pq9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def zero_out_feature(records, feature):\n",
    "    # Description: Set the feature missing in records, by setting the appropriate columns of records to 0\n",
    "    #/ Idea: Randomly zero out inputs using dropout to force the autoencoder to learn useful features\n",
    "    start_index = cat_index[feature]\n",
    "    stop_index = cat_index[feature] + len(cat_values[feature])\n",
    "    records[:, start_index:stop_index] = 0\n",
    "    return records\n",
    "\n",
    "def zero_out_random_feature(records):\n",
    "    # Description: Set one random feature missing in records, by setting the appropriate columns of records to 0\n",
    "    return zero_out_feature(records, random.choice(catcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CCK05iYIubbd"
   },
   "outputs": [],
   "source": [
    "def data_loader(bs=64, _shuffle=True):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=bs, shuffle=_shuffle)\n",
    "    valid_loader = torch.utils.data.DataLoader(val_data, batch_size=bs, shuffle=_shuffle)\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "u-ZcprrAubbd"
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs=5, learning_rate=1e-4, batch_size=64, plot=True):\n",
    "    ## Load data\n",
    "    train_loader, valid_loader = data_loader(bs=batch_size)\n",
    "\n",
    "    ## Initialize list\n",
    "    iters, train_losses, val_losses, train_accs, val_accs = ([] for i in range(5))\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(\"num_epoch       train_loss       val_loss       train_acc       val_acc\") # table header\n",
    "\n",
    "    ## Training\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            datam = zero_out_random_feature(data.clone()) # zero out one categorical feature\n",
    "            recon = model(datam)\n",
    "            train_loss = criterion(recon, data)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        ''' Answer: Training'''\n",
    "        #$ Validation\n",
    "        for data in valid_loader:\n",
    "            datam = zero_out_random_feature(data.clone()) # zero out one categorical feature\n",
    "            recon = model(datam)\n",
    "            val_loss = criterion(recon, data)\n",
    "\n",
    "        #$ Record values\n",
    "        train_acc = get_accuracy(model, train_loader)\n",
    "        val_acc = get_accuracy(model, valid_loader)\n",
    "\n",
    "        iters.append(epoch + 1)\n",
    "        train_losses.append(float(train_loss))\n",
    "        val_losses.append(float(val_loss))\n",
    "        train_accs.append(get_accuracy(model, train_loader))\n",
    "        val_accs.append(get_accuracy(model, valid_loader))\n",
    "\n",
    "        #$ Print statistics\n",
    "        # use '_' as filler because in some font ' ' doesn't take the same space as a character, so there could be mismatch that is hard to solve\n",
    "        print(\"{0:_^9}       {1:_^10.2f}       {2:_^8.2f}       {3:_^9.2f}       {4:_^7.2f}\".format(str(epoch + 1), train_loss, val_loss, train_acc, val_acc))\n",
    "\n",
    "        #$ Save model data and statistics\n",
    "        model_path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(model.name, batch_size, learning_rate, epoch + 1)\n",
    "        torch.save(model.state_dict(), \"./data/Lab4/\" + model_path)\n",
    "\n",
    "    ## plotting\n",
    "    if plot:\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(iters, train_losses, label=\"Train\")\n",
    "        plt.plot(iters, val_losses, label=\"Validation\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"--------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Final round Traing Loss: {}\".format(train_losses[-1]))\n",
    "    print(\"Final round Validation Loss: {}\".format(val_losses[-1]))\n",
    "\n",
    "    if plot:\n",
    "        plt.title(\"Accuracy Curve\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.plot(iters, train_accs, label=\"Train\")\n",
    "        plt.plot(iters, val_accs, label=\"Validation\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Final round Traing Accuracy: {}\".format(train_accs[-1]))\n",
    "    print(\"Final round Validation Accuracy: {}\".format(val_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKk01pwx1pq_"
   },
   "source": [
    "### Part (b) [3 pt]\n",
    "\n",
    "While plotting training and validation loss is valuable, loss values are harder to compare\n",
    "than accuracy percentages. It would be nice to have a measure of \"accuracy\" in this problem.\n",
    "\n",
    "Since we will only be imputing missing categorical values, we will define an accuracy measure.\n",
    "For each record and for each categorical feature, we determine whether\n",
    "the model can predict the categorical feature given all the other features of the record.\n",
    "\n",
    "A function `get_accuracy` is written for you. It is up to you to figure out how to\n",
    "use the function. **You don't need to submit anything in this part.**\n",
    "To earn the marks, correctly plot the training and validation accuracy every few \n",
    "iterations as part of your training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bHWLfCzM1pq_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    # Description: Return the \"accuracy\" of the autoencoder model across a data set. That is, for each record and for each categorical feature, we determine whether the model can successfully predict the value of the categorical feature given all the other features of the record. The returned \"accuracy\" measure is the percentage of times that our model is successful.\n",
    "    \n",
    "    # Param1 {model} : the autoencoder model, an instance of nn.Module\n",
    "    # Param2 {data_loader} : an instance of torch.utils.data.DataLoader\n",
    "\n",
    "    # Example (to illustrate how get_accuracy is intended to be called. Depending on your variable naming this code might require modification.)\n",
    "    # >>> model = AutoEncoder()\n",
    "    # >>> vdl = torch.utils.data.DataLoader(data_valid, batch_size=256, shuffle=True)\n",
    "    # >>> get_accuracy(model, vdl)\n",
    "\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    for col in catcols:\n",
    "        for item in data_loader: # minibatches\n",
    "            inp = item.detach().numpy()\n",
    "            out = model(zero_out_feature(item.clone(), col)).detach().numpy() #/ produce test result\n",
    "            for i in range(out.shape[0]): # record in minibatch\n",
    "                acc += int(get_feature(out[i], col) == get_feature(inp[i], col))\n",
    "                total += 1\n",
    "    return acc / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxCTlXoV1prB"
   },
   "source": [
    "### Part (c) [4 pt]\n",
    "\n",
    "Run your updated training code, using reasonable initial hyperparameters.\n",
    "\n",
    "Include your training curve in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nj5b71l-1prC",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b4992ab7-90de-41d9-d1ad-34fd52319722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch       train_loss       val_loss       train_acc       val_acc\n",
      "____1____       ___0.07___       __0.07__       __0.44___       _0.44__\n",
      "____2____       ___0.07___       __0.07__       __0.45___       _0.45__\n",
      "____3____       ___0.07___       __0.07__       __0.46___       _0.46__\n",
      "____4____       ___0.05___       __0.06__       __0.53___       _0.52__\n",
      "____5____       ___0.06___       __0.06__       __0.55___       _0.55__\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABL7klEQVR4nO3deVhU9fcH8DcDw6YIKCrKrkKipYKymFu5IWpitmGLS4ZoLlmZIGlWZi7lT+lrouKeAuaCSqKCmYrGJswABugMojKCgCsquADn9wc5RQyKMMNlOa/nOU/OvWfuPfcTcrxz73yuFgACY4wxpgYioQtgjDHWdHBTYYwxpjbcVBhjjKkNNxXGGGNqw02FMcaY2nBTYYwxpjbcVBhjjKkNNxXGqpGdnY0hQ4YIsm8XFxccOnQIt27dwo0bNxAfH49JkyYJUgtjz4ObCmMNjLu7O44fP46TJ0+iS5cuaNOmDaZPnw5PT89abU8k4r/mrH4RBwdH1cjOzqYhQ4ZUWa6rq0urVq2iq1ev0tWrV2nVqlWkq6tLAKhNmzYUERFBt27dohs3btCpU6dIS0uLANC8efNIoVBQUVERZWZm0uDBg1XuNyYmhtasWVNtXRMnTqSYmJhKy4iIOnfuTABoy5YttHbtWjp06BDdu3ePvvzyS8rLyyORSKTMHzt2LKWkpBAA0tLSIj8/P5LL5XT9+nXatWsXmZqaCj7+HI0z+J8wjD2nL7/8Eu7u7ujVqxd69uwJV1dXLFiwAADw+eefQ6FQoG3btmjfvj0CAgJARHBwcMDMmTPh4uKCVq1awcPDA5cuXaqybQMDA/Tt2xd79uypU43vvvsulixZAiMjI/z444+4f/8+Bg8eXGl9SEgIAGD27NkYO3YsBg0ahI4dO+LWrVv4+eef67R/1rwJ3tk4OBpiVHemIpfLydPTU/l6+PDhlJ2dTQDom2++of379yvPGp5E586dKT8/n4YMGUI6OjrV7rNjx45ERPTCCy9Um1OTM5Vt27ZVWr948WLatGkTAaCWLVvSvXv3yNramgBQenp6pbMmc3NzevToEWlrawv+/4Cj8QWfqTD2nDp27IjLly8rX1++fBkdO3YEAPzwww+Qy+WIiopCVlYW/Pz8AABZWVmYM2cOvv76axQUFCA0NBQdOnSosu1bt26hrKxM5brnkZOTU+l1SEgIxo0bB11dXYwbNw7Jycm4cuUKAMDGxgbh4eG4desWbt26hYyMDJSVlaF9+/Z1qoE1T9xUGHtOubm5sLGxUb62trZGbm4uAODevXuYO3cuOnfujNdeew2fffaZ8mOn0NBQDBgwADY2NiAiLF++vMq2S0pKEBsbizfeeKPa/d+/fx+GhobK16p++RNRpdcZGRm4fPkyPD09K330BVQ0IE9PT5iamirDwMBAeUyMPQ9uKow9hVgshp6enjK0tbURGhqKBQsWwMzMDG3atMFXX32FHTt2AABGjRqFzp07AwCKiopQVlaGsrIyODg44NVXX4Wuri4ePHiAkpISlJWVqdznvHnzMGnSJMydOxetW7cGAPTo0QOhoaEAgJSUFHTv3h09e/aEnp4evv766xodS0hICGbPno2BAwdi9+7dyuXr1q3DkiVLYG1tDQAwMzPDmDFjajVejAEN4DM4Do6GGNnZ2fRfixcvJj09PQoMDKTc3FzKzc2lwMBA0tPTIwA0Z84cys7Opnv37lFOTg4tWLCAANBLL71E8fHxVFRURDdu3KCIiAjq0KFDtft2cXGhyMhIun37Nt24cYPi4uLogw8+UK4PCAigwsJCunLlCr333ntVrqksXry4yjatrKyorKyMfvvtt0rLtbS06NNPP6XMzEwqKioiuVxOS5YsEXz8ORpnaP39B8YYY6zO+OMvxhhjasNNhTHGmNpwU2GMMaY23FQYY4ypjY7QBQipoKCg0pfYGGOMPZuNjQ3atWuncl2zbiqXL1+Gi4uL0GUwxlijkpiYWO06/viLMcaY2nBTYYwxpjbcVBhjjKkNNxXGGGNqw02FMcaY2nBTYYwxpjbcVBhjjKkNN5Va0NfXxapVH8HevqPQpTDGWIPCTaUW+vTpgqnTPJGRGYRfd/ujTx97oUtijLEGgZtKLeQ91MeWrLbYGZGOIUN6IiHx/xB97DsMHdpL6NIYY0xQ3FRqIVuSgoSjMbje+RW853cIX8zdjK5dLREVvRiJZ1fhrbf6QyTioWWMNT/8m68WykvLsMPvK6REHcew2TOQWKiHzp0+wkdTfkLLlvrY9asfMjKDMHXqCOjpiYUulzHG6pXgzzQWKhITE+v0fpGONn3w43e0Mi2WBk7wrlgmEtHrr/eluPiVVE4RlJu3nebNe4NatTIU/Hg5ODg41BHP+N0pfIENdGBqFKoay5N45ZWX6PCRb6icIujW7TBaunQimZubCn7cHBwcHHUJbiq1G5gax9MaCwDq1asThYR+QY9L91PJg320bt0M6ty5g+DHz8HBwVGbEKypeHh4UGZmJslkMvLz81OZExgYSDKZjFJSUsjJyYkAkIODA0kkEmXcuXOHPvnkEwJAK1asoIyMDEpJSaF9+/aRsbExASAbGxsqLi5WvicoKKiuA/Nc8e/GMmjCeJU5nTqZ09q106m4ZC89Lt1PYbv8yNm5s+A/IBwcHBzPE4I0FZFIRHK5nOzs7EgsFpNUKiVHR8dKOZ6enhQZGUkAyM3NjeLi4lRuJy8vj6ytrQkADRs2jLS1tQkALVu2jJYtW0ZARVNJS0tT58A8/zHXoLEAoPbtTWjJkg/o1u0wKqcIOhr1LQ0e3EPwHxQODg6OmsTTfndq7O4vV1dXyOVyZGdn4/HjxwgLC4OXl1elHC8vL2zfvh0AEB8fDxMTE5ibm1fKGTJkCLKysnDlyhUAQHR0NMrKygAAcXFxsLS01NQhPLfy0jLs9FsE6dHfMeaL2Rg0YbzKvPz82/jyy19gbTUZ877YjBdftMGx35cgPuH/8MYbL/PtyIyxRktjv70sLCyQk5OjfK1QKGBhYfHcOd7e3ggNDVW5jw8//BCHDx9Wvrazs0NycjJOnDiB/v37q3yPj48PEhMTkZiYCDMzs+c+rmcpL6vcWF6Z+G61uXfvluDHH8PRye4jTPX5H4yNDbF7z3ykZ6zFRx8Nh65us37aM2OsEdJYU9HS0qqyjIieK0csFmPMmDHYvXt3lbyAgACUlpZi586dAIC8vDxYW1vD2dkZn332GUJCQmBkZFTlfcHBwXBxcYGLiwuuX7/+3MdVE8rGcuQYXps766mNBQAePnyMjRuj0M3xY7z15lIUFRVjQ/AsXMzeiLlzX4eRkYFG6mSMMXXTWFNRKBSwsrJSvra0tERubu5z5Xh6eiI5ORkFBQWV3jdhwgSMHj0a7733nnLZo0ePcPPmTQBAcnIysrKy4ODgoNZjeh7lZWXY6f91jRsLAJSXl2Pv3j/h6vIZhg75En/9dQUrfvgQl69sxpIlH6BdOxPNF84YY3WkkQs52tralJWVRba2tsoL9d26dauUM3LkyEoX6uPj4yutDw0NpUmTJlVa5uHhQX/99ReZmZlVWm5mZkYikYgAkJ2dHSkUCjI1ffp3QtR9oV5ViLS16YMfFtPKtFh6ZdJ7z/1+Z+fOFLbLj0rLDlBxyV5au3Y6depkLviFOg4OjuYbgt1S7OnpSefPnye5XE4BAQEEgHx9fcnX11eZs2bNGpLL5ZSamkq9e/dWLjcwMKDr169Tq1atKm1TJpPRlStXqtw6PG7cODp37hxJpVJKSkqi0aNH13Vg1BYibW16f8W3tW4sAKhLlw60bt0MKnmwjx6X7qeQ0C+oV69Ogv9wcXBwNL/gLz/WbmDUGupoLADI3NyUli6dqLwd+fCRb+iVV14SfCw5ODiaT3BTqd3AqD3U1VgAUKtWhjRv3huUm7edyimC4uJX0uuv9yUtLS3Bx5WDg6NpBzeV2g2MRkKkrU3vL/+GVqbF0quT69ZYAJCenph8fDzogmw9lVMEpWcE0YcfDiNdXR3Bx5eDg6NpBjeV2g2MxkLdjQWomHngrbf6U+LZVVROEZSj2Eqff/46tWxpIPg4c3BwNK3gplK7gdFoaKKxPImhQ3tRVPRiKqcIunEzlBYvfp/atjUWfLw5ODiaRnBTqd3AaDxE2tr03pPG8uH7at9+nz729OtufyotO0D3i/fQmjXTyNa2veDjzsHB0biDm0rtBqZeQtONBQDZ23ekDRtmUsmDffTo8X7asXMu9ehhK/ixc3BwNM7gplK7gam3qI/GAoA6dGhNy5dPott3dlE5RdBvhxbRwIEvCn78HBwcjSu4qdRuYOo1RNra9N6yr2llWiwNnvKBRvdlbNyC/P3fpLxrFbcjn/nzB/LycufbkTk4OGoU3FRqNzD1HvXZWACQvr4u+fqOIJl8A5VTBP2VvpYmThxCYjHfjszBwVF9cFOp3cAIEvXdWACQtraI3nlnACUlr6ZyiqDLVzbTnDle1KKFvuDjwcHB0fCCm0rtBkawEKKxPInhw53o2O/fUTlF0PUbIfTNN++RmVmreq2Bg4OjYQc3ldoNjKBRubFMqPf9u7jY0+4986m07ADdu7+HfvppKtnYtBN8XDg4OIQPbiq1GxjBQ0skErSxAKAXXrCkjRtn0YOHFbcjb//lM3rxRRvBx4aDg0O44KZSu4FpENEQGgsA6tixNf3ww4d0p6jiduSI376i/v27CVYPBweHcMFNpXYD02BCSySid5cuopVpsTTko4mC1mJi0oICAt6ma/m/UDlFUMzp5fTaa658OzIHRzMKbiq1G5gGFQ2psQAVtyNPnz6Ssi5upHKKoLRzP9OECYNJR0db8No4ODg0G9xUajcwDS4aWmMBKm5H9vYeSBLpT1ROEXTp8maaPfs1MjTUE7w2Dg4OzYRgTcXDw4MyMzNJJpORn5+fypzAwECSyWSUkpJCTk5OBIAcHByUjwuWSCR0584d+uSTTwgAmZqaUlRUFF24cIGioqLIxMREuS1/f3+SyWSUmZlJw4cPr+vANMjQEolo/PdfVTQWn4bRWJ7EiBG96fgf31M5RVDh9Z20aNF4atOGb0fm4GhqIUhTEYlEJJfLyc7OjsRiMUmlUnJ0dKyU4+npSZGRkQSA3NzcKC4uTuV28vLyyNramgDQ8uXLlQ3Kz8+Pli1bRgDI0dGRpFIp6erqkq2tLcnlchKJRHUZmAYbDbmxVPy/fIH27gugcoqgu/d20+rVPmRt3Vbwujg4ONQTT/vdqQMNcXV1hVwuR3Z2NgAgLCwMXl5eyMjIUOZ4eXlh+/btAID4+HiYmJjA3Nwc165dU+YMGTIEWVlZuHLlivI9r7zyCgBg27ZtOHHiBPz9/eHl5YWwsDA8evQIly5dglwuh6urK+Li4jR1iIKh8nKELfgOADBy9jQAwO/B24QsqZL4+PN4Y9z36NrVEl/MewPTPx6JGTNH4ebNe7h37wHu3Sv5z38f4L6KZary/r3s0aNSoQ+VMfYfGmsqFhYWyMnJUb5WKBRwc3N7Zo6FhUWlpuLt7Y3Q0FDl6/bt2yvXX7t2De3atVNu698N5Mm2/svHxwdTp04FAJiZmdXlEAWlbCzUMBsLAGRmKjDlw0As+monJk0agvbtTdCipQFattT/OwzQtq2x8s9P/ltTjx+XPrXp3K9hc/r3spKSRyAiDY4KY02bxpqKlpZWlWX//cv6rByxWIwxY8Zg/vz5atkfAAQHByM4OBgAkJiY+MztNmRUXo6whf+csWhpaeHYhq3CFqWCQnEd3323q0a5WlpaaNFCv1Ljqfxf1cta/GtZx46tq6zX1tau0f7Ly8tx//4DFc3n70Z1/2GtzqrKysrrMoSMNRoaayoKhQJWVlbK15aWlsjNzX2uHE9PTyQnJ6OgoEC5LD8/X/kRmbm5uXJdTfbXFP27sXjO8gWABtlYaoqI/v5lXKLW7err69aoOf17WYt/LWvduiWsrdtWytPTE9d4/w8ePHpq8/nvWdXt2/cRFnYK9+8/UOs4MKZpGmsqiYmJsLe3h62tLa5evQpvb2+8++67lXIOHjyImTNnIiwsDG5ubrhz506lj77Gjx9f6aOvJ++ZOHEili9fjokTJ+LAgQPK5SEhIfi///s/dOzYEfb29khISNDU4TUoTxoLETWJxqIJDx48woMHj3D9epHatikW66BFC70an03906j+WfbPx39/r2+hr9z+kKE98e74H9RWL2P1RWN3CHh6etL58+dJLpdTQEAAASBfX1/y9fVV5qxZs4bkcjmlpqZS7969lcsNDAzo+vXr1KpV5VtSW7duTceOHaMLFy7QsWPHyNTUVLkuICCA5HI5ZWZm0ogRI+p0B0NjDC2RiLy/W0gr02JpqO9kwevheP4QiUTUsqUBLVs2kcopgpydOwteEwfHf4O//Fi7gWmUwY2laUSrVoZUULiTjkZ9K3gtHBz/jaf97hSBNSlUXo5dXy1B4oFIeM6ciqG+k4UuidVCUVExlny3C8OGOWHo0F5Cl8NYjXFTaYL+aSyH4DlzKoZN+1DoklgtBAVF4tKlfCxbPknl3Y2MNUTcVJqoisbyPRIPHMKIGT7cWBqhR49KsXDBDjg7d8Y77wwQuhzGaoSbShPGjaXxCwk5Can0Ir5b8gHEYo3drMmY2nBTaeKeNJaE/b9hxAwfDOfG0qgQEeb7b0OnTubw9R0hdDmMPRM3lWaAysvx66KlSNj/Gzy4sTQ6R48m4/jxFCxY+M5zTWPDmBC4qTQT3FgaN3+/bWjXzgSffz5W6FIYeypuKs2IsrGEc2NpbM6eleHXX0/j87mvo107E6HLYaxa3FSamYrG8v0/jWX6FKFLYjW0cMEv0NfXxcKF7whdCmPV4qbSDBHRP43l44+4sTQSMlkuNgYfxVTfEejcuYPQ5TCmEjeVZoobS+P07bdhePSoFIu/e1/oUhhTiZtKM8aNpfG5du0WVv3ffnh7D0Tv3l2ELoexKripNHPcWBqfH37Yh+vXi7Bs+SShS2GsCm4qTNlY4vdFcGMRkK6BProN6o9xX87FF+E70dNjiMq8u3dL8N3iMAwZ0hPDhjnVc5WMPR3P+8AAVDSW3V8vBQB4fPwRtLS0cHTtRoGravra2lqja/++cBzQF537OEFHVxcPi4vxsLgEY76Yjb9OnEbpw4dV3rdu3WF8MscLy5ZPxLFjUpWPzmZMCNxUmNK/G8uTsxVuLOol1tdDF5fe6DqgopG0sbQAAFzLysbp0D3IjInFxeQU2PTojhlbgzDg3Tfxx5adVbZTMdnkL9ixcy68vQciNPRkfR8KYypxU2GVcGNRPzNrSzgOeBld+/dFZxcniPX08LC4BPKEJPyxZScyT8fiVu61Su+5mCRF+qkzGPzRBMTtPYiSortVthsaegpzvxiH75Z8gL17z+DRo9L6OiTGnkpjTwfz8PCgzMxMkslk5OfnpzInMDCQZDIZpaSkkJOTk3K5sbEx7d69mzIyMig9PZ3c3d0JAIWFhZFEIiGJRELZ2dkkkUgIANnY2FBxcbFyXVBQUJ2eXtbcQ0tLi97+ej6tTIslj48/EryexhQ6enrUtb87jfX/lOYf2k0r02JpZVos+R0MozHzPiGHvi6ko6v7zO10cOhMP6ScoVFzplebM3y4E5VTBM2a9Zrgx83RfEKQxwmLRCKSy+VkZ2dHYrGYpFIpOTo6Vsrx9PSkyMhIAkBubm4UFxenXLd161aaMmUKASCxWEzGxsZV9vHjjz/SwoULCahoKmlpaeocmGYflRrLDB/B62nI0cbSgvq/+yZ9tHYlLUs8QSvTYmlpwh80Zc2P1M/7DWpt2bFW2x3//Ve0LPEEGbdvW21O9LHvKL9gBxkZGQg+DhzNIwRpKu7u7nTkyBHla39/f/L396+Us27dOvL29la+zszMJHNzczIyMqKLFy8+cx9XrlyhLl26EMBNRVPBjUV16Ojq0gsvu5HXvDnkH7FLeTbiH7GLvPzm0Asvu5GOnl6d99PaogMtTz5Fby3yrzanTx97KqcI+uab9wQfF47mEU/73amxayoWFhbIyclRvlYoFHBzc3tmjoWFBUpLS1FYWIgtW7agZ8+eSEpKwieffILi4mJl7oABA5Cfnw+5XK5cZmdnh+TkZBQVFWHBggU4ffq0pg6v2SAi7P5mGYhIOQHl0Z+DBa5KGK0tO8Kxf1907d8XXVx7Q9dAH48fPIQ8MQmnQ3cjIyYON3IUat3nzat5+DNsH/q/+yZObg9FQfblKjlnz8qwa1cMPv3MC2vXHkJ+/m211sDY89BYU1H1TO3/3vZYXY6Ojg6cnZ0xa9YsJCQkYPXq1fD398dXX32lzBs/fjxCQ0OVr/Py8mBtbY2bN2/C2dkZ+/fvR/fu3XH3buULnD4+Ppg6dSoAwMzMrE7H2FwQEfZ8uxwAMHzah9DS0sKRNRsErkrztMVidO7TC10HvAzH/n3Rzs4GAHD9igIJ4RHIiPkT8kSJylt+1elY8Fa4vj4anrN8se2zAJU5Cxf8gnHj+mLhQm/MnLlOo/Uw9jQaayoKhQJWVlbK15aWlsjNza1RDhFBoVAgISEBALBnzx74+/sr87S1tTFu3Dj07t1buezRo0e4efMmACA5ORlZWVlwcHBAUlJSpX0GBwcjOLjiX9qJiYlqOtqm79+NZZjvZABoko3FtKN5xfdG+vdFF7c+0DM0wOOHD5GVKMGfu/YhI+ZPXL+i3rORZ7l/6zZObAvBiBk+sH6pG66kpVfJkcvzELzhKHymemD16gOQy/PqtUbG/k0jn7lpa2tTVlYW2draKi/Ud+vWrVLOyJEjK12oj4+PV647deoUOTg4EABatGgRrVixQrnOw8ODTpw4UWlbZmZmJBKJCADZ2dmRQqEgU1PTWn8uyKE6tLS06M1FfrQyLZZGzJwqeD11DW2xmOzd+tBrc2fRF/tDlNdGAg7vodcDPifHAS+TWL/u10bqGroGBvT1iUM0fdOaanPatzehoru/UmjYPMHr5WjaIciFeqDi7q7z58+TXC6ngIAAAkC+vr7k6+urzFmzZg3J5XJKTU2l3r17K5f37NmTEhMTKSUlhcLDw8nExES5bsuWLZW2AYDGjRtH586dI6lUSklJSTR69Oi6DgxHNVGpscxqfI3FxLw9ub81liYHLqPv43+nlWmxtDzpJE1dv5oGvP8OtbW1FrxGVdFv/Ju0Mi2WXujnXm3ON9+8R+UUQX362AteL0fTjaf97tT6+w/NUmJiIlxcXIQuo1HS0tLCG1/NQ983xyJ6wxYc+V/D/ShMW0cHtk49/v4Cojs62HcGUHERPCPmT2SejoM84SwelTwQuNKn09bRwbyDoXh4vxir3p6kcmoWIyMDyLOCkZZ2CUOHLBCgStYcPO13J3+jntUKEWHvtysAAoZN/fsaSwNqLMbt2/49p9bLsHfvA/0WLVD6+DEuJkmRsP83ZMbEqryTqiErKy3FkTXBeH/5N+jlOQySyKgqORWTTe5C4E9TMXy4E6KiJAJUypozPlPhM5U60dLSwhsL56HvW8KesYh0tGHbqwccB1Tc8tvRoeJZIzdz85B5Og6Zp2Mhj0/Cw3/dlt4YaWlp4dNdW6Fv1ALLX/NGWWnVqVl0dXWQnhGEoqJi9Haew5NNMrXjMxWmMUSEvYtXAKg4Y9GCFg7/b3297LtVu7bo2s8djgP6wt7dBQZGLVH6+DGyk1MQ8eP/kHE6FvlZ2fVSS30hIhwKDMLUdavQ9+2xOB2yp0pOxWSTO7AzZC7Gjx+IkBCebJLVH24qrM6eNBYiwtCpkwBAI41FpKMN254voWv/vuja3x0WXR0AALev5UN69BgyY2Ihizvb6M9GnuX8mTjI4s9i6NTJSNwfqfJ4w8JOYe4Xr2Pxdx9gzx6ebJLVH24qTC2ICPu++wEA1NpYjMzaoGt/dzgOeBkO7i4waGWEsselyJak4Lf/W4OM03G4Jsuq834am0OrgzAndBMGTfBG1LrNVdYTEfz9tuFo1LeYNs0TP/0UIUCVrDnipsLUpkpj0dLC4Z+e79vdIm1t2PTorrzIbuFYcTZyJ78QqdF/ICPmT8jiz+LBvfvqLr9RyTmXjpSo4xg06V38+Ws47t28VSUnOlqCY8ekWLDQG1u3/o6ioqZ9BscaBm4qTK0qNRafiQDwzMbSso0puvareGiVw8uuMGzVCmWlpbgkTcOh1WuREROLvAvyp26jOTr8v/V4cfBADPGZiAPLV6vMme+/DYlnV2Hu3Nfx1VdVH/bFmLpxU2Fqp2wsRCobi5ZIBJuXuqPrgL7oOqAvrLp1BQDcKShE2rGTyDwdiwtxiXhw954g9TcWhZeuIGH/b3j5nXGI2bELN69WnZolKUmOsLBT+PSzsVi7NhLXrlU9o2FMnbipMI0gIuxb8iOAijMWbR0d5F2Q/3024oYWJsYoLyvDJWkaIgPXISPmT+SelwlcdeMTFbQZfUZ7wmOGD0IDvlWZs3DBDrzxxsv46itvfPxxUD1XyJobbipMY/7dWF6d/B4AoOj6Dfx1IgaZp+NwITZB5WNyWc0VFRQiZucuvDL5fZzYGqLyY8KsrDxsWH8EvtM8sWrVAchkuSq2xJj6CD6PjFDBc3/VT2hpaVHXAX3JwtGBtLS0BK+nqYVBKyNafOYofbR2ZbU57dpVTDYZtkv1Y705OJ4nnva7UwTGNIyIkBkTi6sZF/jb3RpQUnQXxzduh+OAl9Gpj5PKnIKC21j5Yzjefrs/XFzs67lC1pxwU2GsCYgJ2YPb+QUYPefjanNWrtyPgoLbWLZ8Uv0VxpodbiqMNQGlDx/i6M8bYdPzRbw4eJDKnHv3SrD42zC8+moPeHg413OFrLngpsJYE3H2YCTyL17CyE+mQaStrTJnw4ajyMrKw7Llk1Q+zpuxuuKmwlgTUV5WhsjAdWjfyRZ9xoxUmfP4ccVkkz172uHdd1Wf0TBWF9xUGGtCzh0/iUspafCY8RF09PRU5uzaFYPk5Cws/u596OrytwqYenFTYayJObQ6CCbt26H/+DdVrq+YbHIrbG3bY/p01Wc0jNWWRpuKh4cHMjMzIZPJ4OfnpzInMDAQMpkMKSkpcHL653ZIY2Nj7N69GxkZGUhPT4e7uzsAYNGiRVAoFJBIJJBIJPD09FS+x9/fHzKZDJmZmRg+fLgmD42xBuviWQkyYv7EkI8mwKCVkcqcY8ekiI6W4MsF76BVK8N6rpA1dRr5coxIJCK5XE52dnYkFotJKpWSo6NjpRxPT0+KjIwkAOTm5kZxcXHKdVu3bqUpU6YQABKLxWRsbEwAaNGiRfT5559X2Z+joyNJpVLS1dUlW1tbksvlJBKJav0FHg6OxhwdHLrQDylnaNSc6dXmODt3pnKKoMWL3xe8Xo7GFYJ8+dHV1RVyuRzZ2dl4/PgxwsLC4OXlVSnHy8sL27dvBwDEx8fDxMQE5ubmMDIywsCBA7Fp0yYAwOPHj3Hnzp2n7s/LywthYWF49OgRLl26BLlcDldXV80cHGMNXN4FOSSRURjw3jto1a6typzk5CyEhp7Ep5+Nhbm5aT1XyJoqjTUVCwsL5OTkKF8rFApYWFjUKKdTp04oLCzEli1bkJycjODgYBga/nOKPnPmTKSkpGDTpk0wMTGp8f4AwMfHB4mJiUhMTISZmZm6DpexBufImg3Q0hZh+PQPq81ZuGAHxGJtLFo0vh4rY02ZxpqKqnvg/ztFR3U5Ojo6cHZ2RlBQEJydnXH//n34+/sDAIKCgtC5c2f06tULeXl5WLlyZY33BwDBwcFwcXGBi4sLrl+/XqtjY6wxuHk1D3/u2gfXsaPR1tZaZc7Fi9ewft1hTPloOBwcqv4jjLHnVaOmYmhoqPylbW9vj9deew06Ok+/FVGhUMDKykr52tLSErm5uTXKUSgUUCgUSEhIAADs2bMHzs4V3wAuKChAeXk5iAjBwcHKj7hqsj/Gmpvfg7fh8cOH8JzlW23O4sW7UFLyEN8t+aAeK2NNVY2ayqlTp6Cvr4+OHTvi999/x+TJk7F169anvicxMRH29vawtbWFWCyGt7c3Dh48WCnn4MGDmDBhAgDAzc0Nd+7cwbVr15Cfn4+cnBw4OFQ8SnbIkCFIT08HAJibmyvf//rrr+PcuXPKbXl7e0NXVxe2trawt7dXNiXGmqt7N2/h5NYQ9Bw+GFYvdlOZU1h4Byt/DMebb/aDm9sL9Vwha4qeeaU/KSmJANDMmTPpiy++IACUnJz8zPd5enrS+fPnSS6XU0BAAAEgX19f8vX1VeasWbOG5HI5paamUu/evZXLe/bsSYmJiZSSkkLh4eFkYmJCAGj79u2UmppKKSkpdODAATI3N1e+JyAggORyOWVmZtKIESPqdAcDB0dTCT1DQ/r6xCGatvF/1ea0bGlAede20/E/vhe8Xo6GH8/43fnsDSQnJ5O7uzvFxsZSt27dCAClpqYKfmAaHhgOjiYT/d99k1amxdILL7tVm/PxxyOpnCJoxIje9VYXR+OMOt9SPGfOHMyfPx/h4eFIT0+HnZ0d/vjjj5q8lTHWAMT+uh83FFcxas7H1U4kGRwcBbk8F8uWT4RIxJNtsNp7rg6lpaVFRkZGgndKdQSfqXA0p3AeNZxWpsWS08jh1ea8/XZ/KqcIev/9VwWvl6PhRp3PVHbu3AkjIyMYGhoiPT0d58+fx9y5c2vyVsZYAyGJjMbVzAsYMdMH2tXcvbl79xmcPSvD4u/eh56euJ4rZE1BjZpKt27dcPfuXYwdOxaRkZGwtrbGBx/w7YeMNSZEhEOrg2BmZQn3t8ZWm+PvtxU2Nu14sklWKzVqKmKxGDo6Ohg7diwOHDiA0tJSftY4Y43Q+TNxkCckYZjvZOgaGKjMOX48FUePJuPLBW/zZJPsudWoqaxfvx6XLl1CixYtcOrUKVhbW6OoqEjTtTHGNODQ6rUwatMagyZWPzXLfP9taNOmFebNe6MeK2NNRa0u1Ghrawt+saiuwRfqOZprTPy/72lJ3DFqYWpSbc6OnXPp3v091KFDa8Hr5WhYUecL9a1atcLKlSuVEzH++OOPaNGiRU3eyhhrgA7/bz109fUx1GdStTkLF/wCHR0RTzbJnkuNmsrmzZtx9+5dvP3223j77bdRVFSELVu2aLo2xpiGFGRfRkL4b3j5nddh2tFcZU52dj7WrzuCD6cMwwsvWNZzhawxe+apjkQiqdGyxhb88RdHc45W7drSssQTNH7JV9XmtG1rTHeKdtHuPfMFr5ej4USdP/4qKSlBv379lK9ffvlllJSU1OStjLEGqqigEDEhv8J5tAc6OHRWmVNYeAc//hCON954mSebZDX2zK7Uo0cPkkqllJ2dTdnZ2ZScnEwvvfSS4N2yrsFnKhzNPQxaGdHiM0dpys8/VpvTooU+5V3bTidOLhW8Xo6GEXU+U0lNTUWvXr3Qo0cP9OjRA87Ozhg8eHBN3soYa8BKiu7i+Mbt6DawHzr17qUy5/79B/j2mzAMHPgiRo7sU78FskbnuWaNu3v3Lu7evQsA+OyzzzRSEGOsfsWE7MHt/AKM+vTjanOCg49CJsvF0mU82SR7ulr/dFQ30yljrHEpffgQUWs3wrbnS3hx8EDVOaVlWPDlL3jpJVu8//4r9Vsga1Rq3VR4mhbGmo7EA5HIv3gJnrOnQaStrTJnz54zSEyU4Ztv3+PJJlm1ntpUioqKcOfOnSpRVFSEjh071leNjDENKy8rw+Gf1sG8sx36vOapMuffk01+/DFPNslUe2pTadWqFYyNjatEq1atIBY/+18qHh4eyMzMhEwmg5+fn8qcwMBAyGQypKSkwMnJSbnc2NgYu3fvRkZGBtLT0+Hu7g4AWLFiBTIyMpCSkoJ9+/bB2NgYAGBjY4Pi4mJIJBJIJBIEBQXVeBAYY0Da7ydxOeUcPGZ8BB09PZU5f/yRiiNHkhDw5dswNuZZNZhqGrnlTCQSkVwuJzs7OxKLxSSVSsnR0bFSjqenJ0VGRhIAcnNzo7i4OOW6rVu30pQpUwgAicViMjY2JgA0bNgw5bxjy5Yto2XLlhEAsrGxobS0NLXdFsfB0Ryjcx8nWpkWS69MfLfanJ497aicImjJkg8Er5dDmKjzLcW14erqCrlcjuzsbDx+/BhhYWHw8vKqlOPl5YXt27cDAOLj42FiYgJzc3MYGRlh4MCB2LRpEwDg8ePHuHPnDgAgOjoaZWVlAIC4uDhYWvL0EYypS9ZZCTJOx2KIz0ToG7VUmZOSko0dO/7AJ3O80LFj63qukDV0GmsqFhYWyMnJUb5WKBSwsLCoUU6nTp1QWFiILVu2IDk5GcHBwTA0rPpchw8//BCHDx9Wvrazs0NycjJOnDiB/v37q6zLx8dHOTGmmZlZXQ+TsSYncnUQDI1bYfCH1T+I76uFO3mySaaSxpqKqluO/3vHWHU5Ojo6cHZ2RlBQEJydnXH//n34+/tXygsICEBpaSl27twJAMjLy4O1tTWcnZ3x2WefISQkBEZGRlW2HxwcDBcXF7i4uOD69et1OUTGmqTc8zIk/XYEA957G63aqv6H16VL+VgXdJgnm2RVaKypKBQKWFlZKV9bWloiNze3RjkKhQIKhQIJCQkAgD179sDZ2VmZN2HCBIwePRrvvfeectmjR49w8+ZNAEBycjKysrLg4OCgkWNjrKk7smYDRDraGD59SrU53323C/fvP8SS7/nR4uwfGmsqiYmJsLe3h62tLcRiMby9vXHw4MFKOQcPHsSECRMAAG5ubrhz5w6uXbuG/Px85OTkKJvCkCFDkJ6eDqDijjI/Pz+MGTOm0qSWZmZmym/62tnZwd7eHhcvXtTU4THWpN28mofYX8Ph+vpotLW1Vplz/XoRfvxhH8aNexnu7jzZJPuHxu4Q8PT0pPPnz5NcLqeAgAACQL6+vuTr66vMWbNmDcnlckpNTaXevXsrl/fs2ZMSExMpJSWFwsPDycTEhACQTCajK1eukEQiIYlEQkFBQQSAxo0bR+fOnSOpVEpJSUk0evToOt3BwMHR3KNla1NaEneMJqxcUm1Oixb6lJvHk002t3jG707hC2ygA8PB0exj+PQptDItlqy6O1abM22aJ5VTBI0a5SJ4vRz1E4LcUswYa/xObgvF3Rs3MWpO9ZNNbtwYxZNNMiX+CWCMVethcTGObdgKe/c+cOjrqjKntLQMXwZsx4sv2uCDD16t5wpZQ8NNhTH2VLG79+OGIhej5nxc7ezke/acQULCBXy7+D3o6+vWc4WsIeGmwhh7qrLHj3Hk5w2w7PYCeo0YWm2ev99WWFm1xYwZo+qxOtbQcFNhjD2T5FAUrmZewIhZU6Gto6My58SJNBw+nIT5AW/xZJPNGDcVxtgzEREiA4NgZmUJ9ze9qs2b778VJiYt4Of3Rj1WxxoSbiqMsRrJPB0HeWIyhk37ELoGBipzUlMvYefOk/hkzhiebLKZ4qbCGKuxQ6vXwqhNawyc4F1tzlcLd0AkEuHrr9+tx8pYQ8FNhTFWY1dS/0LqsRN4ddJ7aGFqojLn8uUCBK2NxOQPh6JrV55ssrnhpsIYey6Hf1oHXQN9DPGZWG3OkiW//j3Z5IR6rIw1BNxUGGPPpSD7MhL3H0K/d8bBtIO5ypwbN4qwYvlevP56X/Tt27WeK2RC4qbCGHtuR4M2gsoJHjN8qs1ZvfoA8vJuYtnySfVXGBMcNxXG2HO7k1+I0yG70fu1Eejg0FllTnHxQ3z7TSgGDOiO0aNd6rlCJhRuKoyxWvl90y94cO8eRs6eXm3Opk3RuHDhKpYum8STTTYT/H+ZMVYrJUVFOL7pF3Qb1A92zj1V5jyZbLJ7d2tMmMCTTTYH3FQYY7V2OmQ37uQXYvSnM6rN2bv3T8THn8c33/Jkk80BNxXGWK09fvAQR4M2wrbXS+j+6oBq855MNjlzJk822dRptKl4eHggMzMTMpkMfn5+KnMCAwMhk8mQkpICJycn5XJjY2Ps3r0bGRkZSE9Ph7u7OwDA1NQUUVFRuHDhAqKiomBiYqJ8j7+/P2QyGTIzMzF8+HBNHhpj7G+J+w+hIPsyRs6eBq1qrpucPHkOkZFnMT/gbZiY8GSTTZ1GHjcpEolILpeTnZ0dicVikkql5OhY+ZGknp6eFBkZSQDIzc2N4uLilOu2bt1KU6ZMIQAkFovJ2NiYANDy5cvJz8+PAJCfnx8tW7aMAJCjoyNJpVLS1dUlW1tbksvlJBKJav1ITA4OjprHS0NfoZVpseQydlS1OT162FJp2QFatmyi4PVy1C0EeUa9u7s7HTlyRPna39+f/P39K+WsW7eOvL29la8zMzPJ3NycjIyM6OLFiyq3+yQHAJmbm1NmZqbK7R85coTc3d3rMjAcHBzPEbN3bqQFUeGko6tbbc7WbZ/S/eI9ZGHRRvB6OWofgjyj3sLCAjk5OcrXCoUCFhYWNcrp1KkTCgsLsWXLFiQnJyM4OBiGhoYAgPbt2+PatWsAgGvXrqFdu3Y13h9jTHMOrV4L0w7m6Odd/bT3i77ayZNNNnEaayqqHjtKRDXK0dHRgbOzM4KCguDs7Iz79+/D39+/zvsDAB8fHyQmJiIxMRFmZmbPOgzGWA1lJSYj83Qchk6dBH2jlipzLl8uwNqfD2HS5CFwdLSq5wpZfdBYU1EoFLCy+ueHxtLSErm5uTXKUSgUUCgUSEhIAADs2bMHzs7OAID8/HyYm1fMN2Rubo6CgoIa7w8AgoOD4eLiAhcXF1y/fl1NR8sYAyrOVgyNW+HVye9Xm/P997tx794DnmyyidJYU0lMTIS9vT1sbW0hFovh7e2NgwcPVso5ePAgJkyo+MFyc3PDnTt3cO3aNeTn5yMnJwcODg4AgCFDhiA9PV35nokTJwIAJk6ciAMHDiiXe3t7Q1dXF7a2trC3t1c2JcZY/cg9L0PyoaMY+P47aNVW9ScBTyabHDvWHS+/7FjPFbL6oLGLOZ6ennT+/HmSy+UUEBBAAMjX15d8fX2VOWvWrCG5XE6pqanUu3dv5fKePXtSYmIipaSkUHh4OJmYmBAAat26NR07dowuXLhAx44dI1NTU+V7AgICSC6XU2ZmJo0YMaJOF5s4ODhqF60tO9Ly5FP0xsJ51eYYGuqR4upWOhWzXPB6OZ4/BLn7qzEENxUODs3E6/M/oxWSGDKzsao2x8fHg8opgl57zVXwejmeLwS5+4sx1nxFb9iC0keP4DnLt9qczZujcf68At8vnQhtbf5V1FTw/0nGmNrdu3ELJ7eFopfHEFh2U/2QrrKycgTMfzLZ5OB6rpBpCjcVxphGnNgWgns3b2HUpx9XmxMeHou4uEyebLIJ4abCGNOIh/eLcWzDVji4u8Chb/UP6fL32wZLSzPMmjW6HqtjmsJNhTGmMX/+Go4bilyMmjND5ReUAeDUqXM4dCgR/vPfgqmp6i9NssaDmwpjTGPKHj/GkZ83wLLbC+jpMaTavID522FsbAh//zfrsTqmCdxUGGMaJTkUhdzzMnjO8oW2jo7KnLS0S/jllz8wa/ZrsLTk6ZMaM24qjDGNIiIcCgyCmbUl3N4YU23eoq92QktLC998w5NNNmbcVBhjGpcZE4ussxIMm/YhdA0MVOZcuVKIn9f8hgkTB6N7d+t6rpCpCzcVxli9+G3Vz2hl1gYDP3in2hyebLLx46bCGKsXV1L/QtrvJ/Hq5PfRwsRYZc7Nm3exfNkejBnjhn79utVzhUwduKkwxupNZGAQdA30McRnYrU5gYERyM29gWXLq89hDRc3FcZYvSnIvozEA5Ho5/0GTDuYq8wpKXmIrxeFoF+/bhgzxq2eK2za2rRphbff7o/g4Fn4/PPXNbIPbiqMsXoVtXYjiAgeM3yqzdmy5RgyMxX4fukEnmyyDvT0xBg8uAeWLp2IxLOrkF/wC8J2+eGNN19G27atNLJP1TeNM8aYhtzOL8DpnbsxaNK7OLEtBNdkWVVyKiab3IZ94V9i4sQh2Lw5WoBKGx8tLS289JINhg1zwtBhvTBwYHcYGOjh8eNSxMZm4utFIYiOluLsWRnKyso1UwMq5sBvlhITE+HiUv2cRIwxzTBo1QoBh3cjOzkVm2d9UW3emT9/gJWVGV5wmIaSkof1WGHj0bFja2UTGTq0J9q3NwUApKdfwbFoKaKjpTh58hzu3StR2z6f9ruTz1QYY/WupKgIf2zegVFzPoadUw9kS1JV5vn7bcXJU8swa9ZorFixt56rbJhatjTAoEEvYtiwXhg6rBe6dav4Tk9+/i0cO5aCY9FSHDsmxdWrNwSpj89U+EyFMUGI9fUw/7fduJmbhzUTqn+Y14GDCzFgQDd07uSDW7fu1WOFDYO2tgh9+tgrm0jfvl0hFuugpOQhTp366++zEQnS0i6DqH5+nT/rd6fGHjnp4eFBmZmZJJPJyM/PT2VOYGAgyWQySklJIScnJ+Xy7OxsSk1NJYlEUunRlWFhYSSRSEgikVB2djZJJBICQDY2NlRcXKxcFxQUVKdHYnJwcGg+3N/0opVpsdT9lf7V5nTvbk2lZQdoxYrJgtdbX9G5cweaNs2T9uydTzdvhVI5RVBp2QFKPLuKli6dSIMH9yA9PbFg9QnyjHqRSERyuZzs7OxILBaTVColR0fHSjmenp4UGRlJAMjNzY3i4uKU67Kzs6lNmzZP3cePP/5ICxcuJKCiqaSlpalzYDg4ODQcIm1t8jsYRnP37SAtkajavM1b5lBxyV6ysmoreM2aCFPTlvTmm/1o/foZlHVxI5VTBJVTBGVf2kTBwbPo7bf7U5s2rQSv80kI0lTc3d3pyJEjytf+/v7k7+9fKWfdunXk7e2tfJ2ZmUnm5uYE1KypXLlyhbp06UIANxUOjsYaPYa9SivTYsnFa2S1OVZWbam4ZC9t2vyJ4PWqI3R1dWjQoBfpu+8+oLj4lVRadoDKKYJu3Q6jfeFf0scfjyR7+46C11ldPO13p8Yu1FtYWCAnJ0f5WqFQwM3N7Zk5FhYWuHbtGogIUVFRICKsX78ewcHBld47YMAA5OfnQy6XK5fZ2dkhOTkZRUVFWLBgAU6fPl2lLh8fH0ydOhUAYGbGU2wzJrTU6D9wJS0dHjN8IDl8DKWPHlXJyckpxM9rDuHTz7yw6v/249y5ywJUWjfdu1sr79IaNOhFtGihj9LSMsTFnce334QiOlqKhIQLGrvVt75orKmoesrbfy8iPS2nX79+yMvLQ9u2bREdHY3MzEzExMQo88aPH4/Q0FDl67y8PFhbW+PmzZtwdnbG/v370b17d9y9e7fS9oODg5UNKjExsfYHyBhTm0Or12L6pjXo5/0GTm4PVZmzdOluTPloGJZ8PwFeYxbXc4XPz9zcFEOH9lLe6tuxYxsAQGamAls2RyM6WooTJ9Jw9676bvVtCDTWVBQKBaysrJSvLS0tkZubW+OcvLw8AEBhYSHCw8Ph6uqqbCra2toYN24cevfurXzvo0ePcPPmTQBAcnIysrKy4ODggKSkJM0cIGNMbeQJScg8HYchPhMRv+8gHty7XyWnYrLJvVi6bCIGDOiOmJi/BKi0eoaGehg48J9bfV96yRYAUFh4B8eOSf++1TcFOTmFwhZaDzTymZu2tjZlZWWRra2t8kJ9t27dKuWMHDmy0oX6+Ph4AkCGhobUsmVL5Z/PnDlDHh4eyvd5eHjQiRMnKm3LzMyMRH9f6LOzsyOFQkGmpqa1/lyQg4OjfsOiqwOtTIslz1m+1eYYGOhRjmIrnfnzB8HrFYlE1KePPc2f/xb9fnwJlTzYR+UUQcUle+lo1Lf0xRfjqFevTqSlpSV4reoOQS7UAxV3d50/f57kcjkFBAQQAPL19SVf339+aNasWUNyuZxSU1Opd+/eBFQ0BalUSlKplM6dO6d875PYsmVLpW0AoHHjxtG5c+dIKpVSUlISjR49uq4Dw8HBUc/x3vJvaGnCH2RkVv1NOlOmDKdyiiAvL/d6r8/Wtj35+HjQrl/9qPD6TuVdWknJq2nFisk0bJgT6evrCj6Omo6n/e7kLz/ylx8ZazDaWFrA72AY4vcdxN7vflCZo60tQmraGgBAj5dmavTCtrFxCwwe3EP5kVaXLh0BAArFdURHS5XfXi8svKOxGhoinqaFMdYo3FBcReye/ej71lic3B6K61cUVXIqJpvcjvD9X2LSpKHYtClKbfsXi3Xg7v6Csom4uNhDW1sbd+8W48SJc/jfT78hOlqCzMyqdbEKfKbCZyqMNSgt25giIHIPMk79iV++WFht3ukzK2Bj0w4O9r51mmzS0dHq7ybihEGDusPIyBBlZWVISJApp0CJizuP0tKyWu+jqeEzFcZYo3Hvxi2c3B6G4dM+xB9bdkCRfl5lnr/fVpyKWY7Zs1/D8uV7arz9du1MMHRoTwwd5oShQ3vC0rLi+2oyWS52/HIC0dES/PFHGu7cqXoHGns2PlPhMxXGGhy9FoYIiNyD3PMyrJ/6SbV5+w8swMCB3dGl81TcvHlXZY6+vi4GDOiu/EirV69OAIAbN4rw+++pOBYtQXS0FJcvF2jkWBoSvRaG6NTbCfbufXD9cg7+3LWvVtvhMxXGWKPy8H4xjgVvw1i/OXDo64ILsaq/qPxlwHZIU37C/Plv4YsvNgOo+FJ1r16dlE2kf/9u0NfXxcOHj3HmTDoC5m9DdLQUEslFlJc37m+vP4uOri5se72ELm69Ye/WB1bdHaGto4PHDx4ids9+zexTI1tljLE6+nPXPgx8/x2MnPMxZHEfqpzW/a+/rmD7tuOYOWs0Ll8uwMv9HDFkSE+0bWsMAEhNzcbanw8hOlqKmJi/UFzctB/0JdLWhmW3F2Dv5gJ7tz6wdXoJYj09lJWWIuevDBzf/AtkcWdxOeWcyulw1IE//uKPvxhrsHqPHoF3ly7CL3MXQHr0d5U5lpZmOH9hHQwM9JCbe6PSrb75+bfrt2ABmNt3hr1rxZlIpz5OMDBqCQDIPS+DLP4sZPFJuJgkwcP7xWrbJ3/8xRhrlJIjo/DK5PcwYpYvUn8/gXIVd2ApFNfh5vo5yssJ6elXBKiyfrW27Ah7tz6wd+uDLq69YdSmNQCg8HIOpEeOQRZ/FlmJybh385Yg9XFTYYw1WFRejsjVQfho7Uq4jRuD2F/DVeY1xlmLa8rIrA26/H0mYu/WB60tOgAA7hQU4vyf8ZAnJEEen4RbedcErrQCNxXGWIOWEfMnspIkGD7tQyRFHMajkgdCl6RR+kYt0cXFWdlIzLtU3K1WXFQEeUIyTmzdCVn8WRRkN8xGyk2FMdbgHVq1FrN3BGPA++/g9+BtQpejVmJ9Pdg59aj4OMutDywdX4BIWxsPi0uQnZyCswcjIYs/i6uZMlAjuFuNmwpjrMG7nHIO546fxKuT30fsr+EovlMkdEm1JtLRhvWL3WHvXnFNxLbni9DR1UXZ41JcTj2H6PVbIIs/iyupf6GstFTocp8bNxXGWKMQGbgOc/ftwBCfiYj48X9Cl1NjWlpa6ODQpeKaiHsfdOrdC3qGhigvL8fVzAuI2fErZAlJyE5OwaOSxv/ALm4qjLFGIf/iJZw9eBj9x7+J0zt3N5gL06qY2Vj9c4eWizNamJoA+OcYZHGJyDoradRnXNXhpsIYazSO/hwMp5HD4DHjI4Qt+E7ocpRatWurbCL2br1hYt4eAHAr7xr+Onka8vgkyBKSUFTQ9J/6yE2FMdZo3M4vwOmQPRg0cTxObA3BNflFQeowNG6FLq69lXdotbOzAQDcu3kL8sRkyDZshSzuLG7kNL8p8rmpMMYald83bof7G2MwcvY0bJ49r172qWtggE69e8LezQVdXHujY1d7iEQiPLh/HxfPShG7Zz9kcWdxTZalcjqZ5kSjTcXDwwOBgYHQ1tbGxo0bsXz58io5gYGBGDlyJIqLizFp0iRIJBIAQHZ2Nu7evYuysjKUlpYqpwRYtGgRfHx8UFhYcRoZEBCAw4cPAwD8/f0xZcoUlJWVYfbs2YiKUt/DexhjDUNJURGOb96BUXOmw86pB7IlqWrfh7ZYDJse3ZUfaVm/1B3aYh2UPnqES9I0HP05GPL4JFz5K13lt/ybO408w1gkEpFcLic7OzsSi8UklUrJ0dGxUo6npydFRkYSAHJzc6O4uDjluuzsbGrTpupzqhctWkSff/55leWOjo4klUpJV1eXbG1tSS6Xk0gkqvVzljk4OBpuiPX16KvfD9LMbevUsj0tkYgsu3WlVz98n6auW0VLE/6glWmx9IP0NM3euZFGfjKd7N1dSEdPT/BjbwjxtN+dGjtTcXV1hVwuR3Z2NgAgLCwMXl5eyMjIUOZ4eXlh+/btAID4+HiYmJjA3Nwc1649/10dXl5eCAsLw6NHj3Dp0iXI5XK4uroiLi5OPQfEGGswHj94iKh1m/HWV37oNqg/0k+efu5ttLOzgb17xWy+nV2cYNiqFQAgT5aFuL0HII8/i6wkKR7cvafu8ps0jTUVCwsL5OTkKF8rFAq4ubk9M8fCwgLXrl0DESEqKgpEhPXr1yM4OFiZN3PmTEyYMAFnz57F559/jtu3b8PCwqJSA3myrf/y8fHB1KlTAQBmZmZqO17GWP1KCI/AKxPGY+Qn05AR8+czv21u2sH872+tV1xgN27XFgBwQ3EVqVF/QJ6QBFnCWdy7cas+ym+yNNZUtLS0qiz77wWsp+X069cPeXl5aNu2LaKjo5GZmYmYmBgEBQVh8eLFICIsXrwYK1euxJQpU2q0PwAIDg5WNqjERNUP/mGMNXzlpWWI/N96TFy5BL1Hj8DZg5GV1rdsbVoxh5Z7H9i79oGZtSUAoOj6jYoGEncW8oSzuHk1T4jymyyNNRWFQgErKyvla0tLS+Tm5tY4Jy+v4n90YWEhwsPD4erqipiYGBQU/PPIz+DgYPz222813h9jrGlJjTqOK+fS4THjI2SejoX1S92VTzns6NAFAFBSdBdZZ5MRs/NXyOLPIj8rW+Cqmz6NXMjR1tamrKwssrW1VV6o79atW6WckSNHVrpQHx8fTwDI0NCQWrZsqfzzmTNnyMPDgwCQubm58v1z5syh0NBQAkDdunWrdKE+KyuLL9RzcDSD6OLam1amxSpjWeIJ8t0QSIOnfEBWL3Yjkba24DU2tRDkQn1ZWRlmzpyJo0ePQltbG5s3b0Z6ejp8fX0BAOvXr0dkZCRGjhwJuVyO4uJiTJ48GQDQvn17hIeHAwB0dHQQEhKCo0ePAgBWrFiBXr16gYhw6dIl5fbS09Px66+/Ij09HaWlpZgxY0aTf/40YwyQJyQh8qd1EOvpQRaXiEsp51D2+LHQZTVb/DhhfpwwY4w9l6f97hTVcy2MMcaaMG4qjDHG1IabCmOMMbXhpsIYY0xtuKkwxhhTG24qjDHG1IabCmOMMbXhpsIYY0xtmvWXHwsKCnD58uVav9/MzAzXr19XY0XqwXU9H67r+XBdz6cp1mVjY4N27dpVu17weWQaazTUucO4Lq6L62o40dzq4o+/GGOMqQ03FcYYY2rDTaUONmzYIHQJKnFdz4frej5c1/NpbnU16wv1jDHG1IvPVBhjjKkNNxXGGGNqw03lGTZt2oT8/HykpaVVmxMYGAiZTIaUlBQ4OTk1iLoGDRqE27dvQyKRQCKRYOHChRqvydLSEsePH0d6ejrOnTuH2bNnq8yr7/GqSV1CjJeenh7i4+MhlUpx7tw5fP311yrzhPj5qkltQowZAIhEIiQnJyMiIkLleiHG61l1CTVWAJCdnY3U1FRIJBIkJiaqzFH3mAl+v3RDjgEDBpCTkxOlpaWpXO/p6UmRkZEEgNzc3CguLq5B1DVo0CCKiIio17EyNzcnJycnAkAtW7ak8+fPk6Ojo+DjVZO6hBgvANSiRQsCQDo6OhQXF0dubm6Cj1dNaxNqzD799FPauXOnyn0LOV5Pq0uosQJA2dnZ1KZNm2rXq3vM+EzlGWJiYnDz5s1q13t5eWH79u0AgPj4eJiYmMDc3FzwuoRw7do1SCQSAMC9e/eQkZEBCwuLSjlCjFdN6hLK/fv3AQBisRhisRhEVGm9UD9fNalNCBYWFhg1ahQ2btyocr1Q4/WsuhoydY8ZN5U6srCwQE5OjvK1QqFoML+w+vbtC6lUisjISHTr1q1e921jYwMnJyfEx8dXWi70eFVXFyDMeIlEIkgkEhQUFCA6OhoJCQmV1gs5Xs+qDaj/MVu9ejXmzZuH8vJyleuFGq9n1QUI9/eRiBAVFYWzZ8/Cx8enynp1jxk3lTrS0tKqsqwh/IsuOTkZNjY26NWrF/73v/9h//799bbvFi1aYO/evZgzZw7u3r1baZ2Q4/W0uoQar/Lycjg5OcHS0hKurq7o3r17pfVCjtezaqvvMRs1ahQKCgqQnJxcbY4Q41WTuoT8+9ivXz/07t0bnp6emDFjBgYMGFBpvbrHjJtKHSkUClhZWSlfW1paIjc3V8CKKty9e1f58cXhw4chFovRpk0bje9XR0cHe/fuxc6dOxEeHl5lvVDj9ay6hBqvJ+7cuYMTJ05gxIgRlZY3hJ+v6mqr7zHr168fxowZg+zsbISFhWHw4MH45ZdfKuUIMV41qUvIn6+8vDwAQGFhIcLDw+Hq6lppvSbGTJCLR40pbGxsqr0gPnLkyEoXueLj4xtEXe3bt1f+2cXFhS5fvlwvNW3bto1WrVpV7XqhxutZdQkxXmZmZmRsbEwASF9fn06dOkWjRo1qEONVk9qE+hkDqr/wLeTfx6fVJdRYGRoaUsuWLZV/PnPmDHl4eGh0zHTAniokJASvvPIKzMzMkJOTg0WLFkEsFgMA1q9fj8jISIwcORJyuRzFxcWYPHlyg6jrzTffxPTp01FaWoqSkhJ4e3trvKZ+/fphwoQJytsXASAgIADW1tbKuoQYr5rUJcR4dejQAdu2bYO2tjZEIhF+/fVXHDp0CL6+vsq6hPr5qkltQoyZKg1hvJ5Vl1Bj1b59e+WZuY6ODkJCQnD06FGNjhlP08IYY0xt+JoKY4wxteGmwhhjTG24qTDGGFMbbiqMMcbUhpsKY4wxteGmwlgdPPlmvo2NDcaPH6/Wbc+fP7/S6zNnzqh1+4xpSr1+OYiDoynF3bt3CajdLLQikahG2+bgaEzBZyqMqcGyZcswYMAASCQSzJkzByKRCCtWrEBCQgJSUlIwdepUABXP1Th+/Dh27typfBZOeHg4zp49i3Pnzikn/Fu6dCkMDAwgkUiwY8cOAKg0X9mKFSuQlpaG1NRUvP3228pt//HHH9i9ezcyMjKU73uyvb/++gspKSn44Ycf6mVMWPMleGfj4GisUd2Zio+PD3355ZcEgHR1dSkxMZFsbW1p0KBBdO/ePbK1tVXmmpqaElAxHUpaWhq1bt260rb/u69x48ZRVFQUiUQiateuHV2+fJnMzc1p0KBBdPv2bbKwsCAtLS36888/qV+/fmRqakqZmZnK7TyZfoWDQxPBZyqMacDw4cMxYcIESCQSxMfHo02bNrC3twcAJCQk4NKlS8rc2bNnQyqVIi4uDlZWVsq86vTv3x+hoaEoLy9HQUEBTp48CRcXF+W2r169CiKCVCqFra0tioqK8ODBA2zcuBGvv/46iouLNXbcjHFTYUwDtLS0MGvWLDg5OcHJyQmdOnVCdHQ0gH8efgVUfGQ1dOhQ9O3bF7169YJEIoG+vv4zt12dhw8fKv9cVlYGHR0dlJWVwdXVFXv37sXYsWNx5MiROh4dY9XjpsKYGty9exdGRkbK10ePHsX06dOho1MxZ6u9vT0MDQ2rvM/Y2Bi3bt1CSUkJXnjhBbi7uyvXPX78WPn+fzt16hTeeecdiEQimJmZYeDAgSofoPVEixYtYGxsjMOHD2POnDno1atXHY6UsafjWYoZU4PU1FSUlpZCKpVi69atCAwMhK2tLZKTk6GlpYXCwkKMHTu2yvuOHDmCadOmISUlBefPn0dcXJxy3YYNG5Camork5GS8//77yuXh4eHo27cvUlJSQESYN28e8vPz0bVrV5W1GRkZ4cCBA9DX14eWlhY+/fRTtR8/Y0/wLMWMMcbUhj/+YowxpjbcVBhjjKkNNxXGGGNqw02FMcaY2nBTYYwxpjbcVBhjjKkNNxXGGGNq8//EFnTwbIKcngAAAABJRU5ErkJggg==",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 404.86875 277.314375\" width=\"404.86875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-03-08T10:31:50.569586</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 404.86875 277.314375 \nL 404.86875 0 \nL 0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 62.86875 239.758125 \nL 397.66875 239.758125 \nL 397.66875 22.318125 \nL 62.86875 22.318125 \nz\n\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8b84d73fad\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"78.086932\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(70.135369 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"116.132386\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(108.180824 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"154.177841\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(146.226278 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"192.223295\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(184.271733 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"230.26875\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 3.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(222.317187 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"268.314205\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 3.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(260.362642 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"306.359659\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 4.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(298.408097 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"344.405114\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 4.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(336.453551 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"382.450568\" xlink:href=\"#m8b84d73fad\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 5.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(374.499006 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Iterations -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(206.455469 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mc8d480b8be\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"226.212128\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.0550 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 230.011347)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"199.890104\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.0575 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 203.689323)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"173.56808\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.0600 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 177.367298)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"147.246055\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.0625 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 151.045274)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"120.924031\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.0650 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 124.72325)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"94.602007\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.0675 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 98.401226)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"68.279983\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.0700 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 72.079201)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"62.86875\" xlink:href=\"#mc8d480b8be\" y=\"41.957958\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.0725 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 45.757177)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- Loss -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pd5e40dfb34)\" d=\"M 78.086932 33.623983 \nL 154.177841 109.62426 \nL 230.26875 87.803243 \nL 306.359659 229.874489 \nL 382.450568 210.072429 \n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pd5e40dfb34)\" d=\"M 78.086932 32.201761 \nL 154.177841 67.749216 \nL 230.26875 71.318575 \nL 306.359659 212.534763 \nL 382.450568 192.019984 \n\" style=\"fill:none;stroke:#feffb3;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 62.86875 239.758125 \nL 62.86875 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 397.66875 239.758125 \nL 397.66875 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 62.86875 239.758125 \nL 397.66875 239.758125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 62.86875 22.318125 \nL 397.66875 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Loss Curve -->\n    <g style=\"fill:#ffffff;\" transform=\"translate(197.499375 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" id=\"DejaVuSans-43\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use x=\"53.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n     <use x=\"115.144531\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"167.244141\" xlink:href=\"#DejaVuSans-73\"/>\n     <use x=\"219.34375\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"251.130859\" xlink:href=\"#DejaVuSans-43\"/>\n     <use x=\"320.955078\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"384.333984\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"425.447266\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"484.626953\" xlink:href=\"#DejaVuSans-65\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd5e40dfb34\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"62.86875\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final round Traing Loss: 0.056532908231019974\n",
      "Final round Validation Loss: 0.058247484266757965\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABCxUlEQVR4nO3deVxN+f8H8Ndd2jct2m4pS6kwCjeyZWxJCGEaBl8zEqbIDMKMMTPmNwtjMMbYhiyDjAkjZN8LXbpRWpSSbkXatKLl8/sjc2eaUqHbaXk/H4/3Q/fes7zukft2zufcc3gAGAghhJD/4HMdgBBCSNNEDYIQQkiNqEEQQgipETUIQgghNaIGQQghpEbUIAghhNSIGgQhhJAaUYMgzdKFCxeQk5MDZWVlrqMojJaWFtauXYuUlBQUFBQgISEBa9euhb6+PtfRSCtBDYI0OxYWFhgwYAAYYxgzZkyjrlsgEDTKepSUlHDu3Dl06dIFI0aMgLa2Nvr27Yvs7Gw4Ojq+9vIaKzdpeRgVVXOq5cuXs6tXr7I1a9aw4ODgKq+ZmZmxoKAglpmZybKystiGDRvkr82cOZPFxMSw/Px8dvfuXebg4MAAMMYY69ixo3y6gIAAtnLlSgaAOTs7s9TUVLZ48WKWkZHBdu/ezdq0acOCg4NZZmYmy8nJYcHBwUwkEsnn19XVZTt27GBpaWksJyeHHT58mAFgUVFRbNSoUfLphEIhe/LkCevevXu19/jRRx+xR48eMQ0NjVduh9fNHRMTw9zc3OTTCwQC9uTJE/l26N27NwsNDWW5ubksMjKSOTs7c/53TcVt0R4EaXamTZuGvXv3Yu/evXBxcYGhoSEAgM/n49ixY0hJSYGlpSVEIhECAwMBABMmTMCXX36JadOmQVtbG2PGjEF2dna91mdsbAw9PT1YWFhg1qxZ4PP5CAgIgIWFBdq1a4eSkhL88ssv8un37NkDdXV1dOnSBYaGhli7di0AYPfu3fjggw/k040cORIZGRm4fft2tXUOHToUJ0+eRFFR0Rtvp//m3r9/P95//3356y4uLsjKyoJUKoWpqSmOHz+Ob775Bnp6eli4cCGCgoJgYGDwxusnLQPnXYqKqr7Vr18/9uLFC6avr88AsNjYWObn58cAsD59+rDMzEwmEAiqzXfy5Ek2b968GpdZ1//Enz9/zlRUVF6ZqXv37iwnJ4cBYMbGxqy8vJy1adOm2nQmJiYsPz+faWlpMQDs4MGDbNGiRTUu8/Tp0+y7776rdVu8bu6OHTuy/Px8pqamxgCw33//nS1fvpwBYIsXL2a7d++uts2mTZvG+d85FXdFexCkWZk+fTpOnz4t/9//vn37MH36dACAubk5UlJSUF5eXm0+c3Nz3L9//43W+eTJEzx//lz+WE1NDZs3b8aDBw/w9OlTXL58Gbq6uuDz+TA3N0dOTg7y8vKqLScjIwOhoaHw8PCAjo4OXF1dsXfv3hrXmZ2dDRMTkzfK+6rc9+/fR2xsLEaPHg01NTWMGTMG+/btA1A5rjNx4kTk5ubKq3///m+dgTRvQq4DEFJfqqqqmDRpEgQCATIyMgAAKioq0NXVxTvvvIPU1FS0a9cOAoGgWpNITU1Fx44da1xuUVER1NXV5Y+NjY0hk8nkjxljVab/9NNP0blzZ/Tu3RuPHz9G9+7dERkZCR6Ph9TUVOjp6UFHRwdPnz6ttq5du3Zh5syZEAqFuHbtGtLT02vMdPbsWXzzzTdQV1dHcXFxg+QGID/MxOfzERMTI2+aqamp2LNnD2bNmlXjukjrxfluDBVVfcrT05NlZ2czc3NzZmRkJK9Lly6xH3/8kfH5fBYZGclWr17N1NXVmYqKCuvbty8DwCZMmMAePnzIevTowYDKwy3t2rVjANjVq1fZd999x/h8PnNxcWHFxcXVBnv/neOHH35gJ06cYCoqKkxXV5cdOnSIMcbkh7aOHTvG9u7dy9q0acOEQiEbMGCAfF5VVVWWk5PDoqKi2NSpU1/5XpWVlVl4eDgLCQlhnTt3Zjwej+np6bGlS5cyV1fXN8oNVB4CKyoqYpcuXapyyM3MzIxlZGSw4cOHMz6fz1RUVJizs3OVwXeqVlmcB6CiqleFhISwH3/8sdrzEydOZBkZGUwgEDBzc3N2+PBhlpWVxZ48ecLWr18vn87b25vFxcWxgoICFhUVxezt7RkA1rNnTxYdHc3y8/PZ7t272b59+2r9oDUxMWEXLlxgBQUFLD4+ns2aNatKg9DV1WU7d+5kjx49Yjk5OSwoKKjK/Nu2bWOFhYW1nqEEgGlra7O1a9eyhw8fsoKCApaYmMjWrFnD9PT03ij333X27FlWWlrKjIyMqjzv6OjILl68yLKzs1lmZiY7duwYMzc35/zvnYq74r38gRDSSJYvXw5ra2tMnTqV6yiE1IrGIAhpRLq6uvjoo4+oOZBmgc5iIqSRzJw5E6mpqQgJCcGVK1e4jkNInegQEyGEkBrRHgQhhJAatagxiMzMTKSkpHAdgxBCmg0LCwv55Wr+q0U1iJSUFIjFYq5jEEJIsyGRSF75Gh1iIoQQUiNqEIQQQmpEDYIQQkiNWtQYRE10dXXh5+cHS0tL8Hg8ruM0a4wxPHjwAOvWrUNubi7XcQghCtbiG4Sfnx9u3ryJr7/+usbLQJP6EwgEcHNzg5+fH1asWMF1HEKIgrX4Q0yWlpY4ceIENYcGUF5ejuPHj8PS0pLrKISQRtDiGwSPx6Pm0IDKy8vpUB0hrUSLbxCEENKSdejlgHdnTFHIslv8GASX9PT0cO7cOQCVd/sqLy/HkydPAACOjo4oLS195bw9e/bEtGnTMH/+/EbJSghpXnRNjDHqUx/YuwxBtiwNV/f/idJnz+ue8TVQg1CgnJwcODg4AABWrFiBwsJCrFmzRv56TbfG/NutW7dw69atRslJCGk+lFRV8O6MDzD4w6ngowJ6SZdhLShu8OYA0CGmRhcQEIA1a9bg/Pnz+OGHHyAWixEaGoqIiAiEhobC2toaAODs7Izg4GAAlc1l+/btuHDhAu7fvw9fX18u3wIhhCP2LkPgfzQQLnNngiVJMVb3PqaPsUNBQTHU1FQafH2tag/CfbEfTG2sGnSZ6XEJ+GvVuteax9raGkOHDkVFRQW0tLQwcOBAlJeXY8iQIfj2228xYcKEavPY2Njg3XffhZaWFuLj47Fp0yaUlZU10LsghDRlpp2tMHbJAnTs5YCC5ARYp8fCbVI33LuXhuHDluPs2UiFrLdVNYim4uDBg6ioqAAA6OjoYNeuXbCysgJjDEpKSjXOc/z4cbx48QLZ2dnIzMyEkZER0tLSGjM2IaSRabTRwQhfb/TxGIOSp0/BkwTDz6M7lJSssfzzPVi9+hBevFDcfxRbVYN43f/pK0pRUZH855UrV+LChQsYP348LCwscPHixRrnef78n+OL5eXlEApb1V8dIa0KXyhAv/c8MHzuR1BRV8eT0DOYMqAtus1wwokTN+HrsxnJyY8VnoM+ZTimo6Mj3xP43//+x20YQgjnrJ3EcF/sB+NOHSC7JUEP1QwsnNsXqalPMH7c/+HIkeuNloUGqTm2atUqfPfdd7h69SoEAgHXcQghHNE3E2HG+u/hvfVnKKkoIzdkD5a66WPSeEesXhUEO9u5jdoc/sZaSkkkkmrP7d69m/NcLa1om1JRNVwpq6kx13mz2Q+3LrFvb5xjH63wYVdDV7MKFswuXvqOdenSTqHrr+lz8++iQ0yEEMIBHo+HHm4ucFswFzqGbRF98hR6aGZh0+dDkZNTgOnTfsKePRc4zUgNghBCGpl5F1uMXboAlt274WFUDJ6E7MPXfiNgYuKArVtO4rPP9iA3t5DrmNQgCCGksWjp62Gk3xw4jh2F/KxshG78GbPG2mD46im4dSsR48f9HySSBK5jylGDIIQQBRMIhRjwwXsY5j0DQhVlhO7ZB4c2+di7biKePXsBX5/N2LQpRP79qKaCGgQhhCiQ7YC+cF88H20t2yHmUigKJGfw/efj0amTKX7//QIWLdyBx4/zuI5ZI4We5uri4oK4uDgkJCTA39+/2uvOzs7Iy8uDVCqFVCrF8uXLq4bj8xERESG/JhEhhDQXbS3bYeavazDz1zVgjCF4xRdwMcpC4E4flJaWY/C7yzBt6k9NtjkACmwQfD4fGzduhKurK+zs7PD+++/D1ta22nRXrlyBg4MDHBwcsHLlyiqvzZ8/H7GxsYqK2CguXLiA4cOHV3lu/vz52Lhx4yun79mzJ4DKy2vo6OhUm2bFihX49NNPa12vu7t7le391VdfYciQIa8bnxDymlQ1NTB6oS8WHdoLS/t3cOynn1F+5SCO7vLCyJG9sGzpLth3n4eLF6O4jlonhTUIR0dHJCYmIjk5GaWlpQgMDIS7u3u95xeJRHBzc8Nvv/2mqIiNYv/+/fD09KzynKenJ/bv31/nvG5ubnj69OkbrXfs2LGws7OTP16xYoX83hSEkIbH4/HgOG40lhz7AwOneuLm0RO4+M3nWLegL1at+h/On7+DLnZz8f33f6K0tHlcaFNhDUIkEiE1NVX+WCaTQSQSVZvOyckJkZGROHHiRJUPtHXr1mHx4sVNbtDmdf35558YNWoUlJWVAQAWFhYwNTXF5MmTIZFIEB0djS+//LLGeZOTk6Gvrw8AWLZsGeLi4nDmzBl07txZPs3MmTMRHh6OyMhI/Pnnn1BTU4OTkxPGjBmD1atXQyqVokOHDggICICHhwcAYPDgwYiIiMCdO3ewfft2ebbk5GR8+eWXuHXrFu7cuVNlPYSQV7Ps3g3z92/He18vQ9ZDGfbM9YVLu2KcPv45tLXV4T5mJca6f4OUlEyuo74WhQ1S13TfYsZYlccRERGwsLBAUVERXF1dceTIEVhbW8PNzQ2ZmZmIiIiAs7Nzrevx8vLCrFmzAAAGBga1Trt27Ux0t+/wmu+kdrcjk7Bgwav3cnJychAeHo4RI0bg6NGj8PT0xIEDB/Ddd98hNzcXfD4f586dQ7du3RAVVfMuZ48ePeDp6QkHBwcIhUJERETIbyZ06NAh+V7WypUr8dFHH+GXX37B0aNHcezYMQQFBVVZloqKCnbu3IkhQ4YgISEBu3btwpw5c7B+/XoAQFZWFnr27Ik5c+Zg4cKF8PLyaojNREiLpG3YFqMWzEXPUSOQ9zgTe5esQE9THs4fXQhNTVV89+0f+L//+wPFxQ1/M5/GoLA9CJlMBnNzc/ljMzMzpKenV5mmoKBAfmXTkJAQKCkpQV9fH/369cOYMWOQnJyMwMBADB48GHv27KlxPdu2bYNYLIZYLEZWVpai3s5b+fdhpr8PL02aNAm3bt2CVCpFly5dquw9/deAAQNw+PBhlJSUoKCgAEePHpW/1rVrV1y+fBl37tzBlClT0KVLl1qzdO7cGcnJyUhIqDzXeteuXRg4cKD89UOHDgGovKOdpaXlm75lQlo0obIyhnhNx5LgA3hn2Ls4szUAJz9fgo3Lh2PLVh/cvp0M++7z8Nlne5ptcwAUuAchkUhgZWUFS0tLpKWlwdPTE5MnT64yjZGRER4/rrxkrVgsBp/PR3Z2NpYtW4Zly5YBqDzTaeHChZg6depbZ6rtf/qKdOTIEfz0009wcHCAmpoacnNzsXDhQojFYuTl5SEgIACqqqq1LuO/e19/27lzJ8aOHYs7d+5g+vTpGDRoUK3LqWnP7t/+vqw4XVKckJp1HeyMMYt8oW8mwp0zF3Bx22/4ZPZQzF37HZ48yccHU37Evn2XuI7ZIBS2B1FeXg4fHx+cOnUKsbGx+OOPPxATEwNvb294e3sDACZMmIDo6GhERkbi559/rjaY21IUFRXh4sWL2LFjB/bv3w9tbW0UFRXh6dOnMDQ0hKura63zX758GePGjYOqqio0NTUxevRo+WtaWlrIyMiAUCjElClT5M8XFBRAS0ur2rLi4uJgaWmJjh07AgCmTp2KS5daxi8zIYpk1LE9vLf9jBnrv8eLkmfYPNMXz8NPIuzcCnzs44bNm0JgazOnxTQHQMFflAsJCUFISEiV57Zs2SL/eePGja883fNvly5dahEfYPv378fhw4fh6emJ+Ph4SKVS3L17F0lJSQgNDa11XqlUigMHDiAyMhIpKSm4cuWK/LXly5fjxo0bSElJQVRUlLwpBAYGYtu2bZg3b16VW5g+f/4cM2bMwMGDByEUCiGRSLB582bFvGlCWgA1bW2M+HgmnCaNw/OiYhz6dg2yI29g48+zMHSoPcLD72H0qK8REXGf66gKwfnlbhuq6HLfjVO0TalaQ/H4fOY0aRz7+nIIWx15lY3/bCEzMGnLVq78gD17fohl5+xn3t4jGJ/P5zzr2xRd7psQQl5Dx14OGLtkAUw7WyEx/BaO/LAWPaz0cCP0O7Rvb4Rdu87Bf/FOZGbmcR1VoahBEELIS7omxhi90Bfdhw9GTnoGdn2yDHnx0fhlnRfGjXPC3bsPMch5KS5fjuY6aqNo8Q2CMQaBQIDy8nKuo7QIAoHglWdUEdJcKamqYPCHU/HujA/AGMPJjdtw9fcD8J07AssP/woA8F8cgLVr/0JZWev5LGnxDeLBgwdwc3PD8ePHqUm8JYFAADc3Nzx48IDrKIQ0GHuXIRj1qQ90TYwhDTmDYz9tRPfObSG5sQp2du1w6FAYFvj9htTUJ1xHbXQtvkGsW7cOfn5+8PDwqPM7AKR2jDE8ePAA69at4zoKIW9NZGMN9yV+6NjTAWmx97B3yZcokqVg/Y8fYurUd5GU9Aij3L7CiRM3uY7KKc5H0RuqahuNp6KiogLANHTbsAlf+LPVt0PZV5dOsD4T3JlAKGRz5oxkObn7WcmzQ+zrr6cwNTUVzrM2RtFZTISQVo8vFKDfex4YPvcjqKir4+regzi9eTvsrIxw7doq9OplhbNnI+Hz8Wbcu5fGddwmgRoEIaTFs3ZyhLu/H4w7tkd86HX8tWo9nmU/wepvp2L2HFc8fpyH9z1X4cCBK3UvrBWhBkEIabH0zUQYs3geur47EFkPZdjhuwh3L17FBx+8i9U/fg0DA21s+DkYK1bsQ35+MddxmxxqEISQFkdFXR1DvKbDeZonysvKcHzdr7i0OxDWnYxx/sK3GDSoG65di8MIlxW4fTuZ67hNFjUIQkiLwePx0MPNBW4L5kLHsC0kf53AifWbUFZUgG++noxPPh2LgoISzPLagO3bz9B3eupADYIQ0iKYd7XD2CV+sOzeDQ+jYrBzwVI8vHMX7u59sG69FywsDLFj+2ksWbILWVn5XMdtFqhBEEKaNS19PYz0mwPHsaOQn5WNwM9X4ubREFhYGOKvo8sxerQj7txJRv9+ixEWFst13GaFGgQhpFkSCIUY8MF7GOY9A0IVZZzfsQdnt+4EK32BpUsn4rPPJ6GsrByffvIbNmw41qoukdFQqEEQQpod24H94L5oHtpatsPdi1dxdPV6ZD2UYfDgd/DLxjmwsTHDwYNX8cmC35CWls113GaLGgQhpNkwbG+BMYvnw7a/EzKTU7B19gLEh16HsbEuft+7EJMnOyMxMR2uI1bg1KkIruM2e9QgCCFNnqqmBobN/hADJk/Ci2fP8Nfq9Qjd9yfAKuDjMworv/kAKipK+HLFXvzwQxCePy/lOnKLQA2CENJk8fh8iN3dMHL+bGjotkH4oWCEbNiCwpxcODpa49dNc9GjR0ecPHkL83y3IDExg+vILQo1CEJIk2Rp/w7GLl0AczsbJEvv4Le5n0AWEw9dXU1s3vwxZnoNR3p6DiZO+A5BQWFcx22RqEEQQpoULQN9jFnoix5uLsh7nInf/VdAeuI0eDwepk8fglWrZ0BXVxNrf/oLX321H4WFJVxHbrGoQRBCmgy+UACvTT/BsL0FzmwJwPntu/Gi5Bm6drXAxl/nYMCALggNjcHcOZsQFfWA67gtHjUIQkiTMXDKexDZWCNg/hJEn78EDQ1VfLNqBvwWuCMvrwgfzliHXbvO0yUyGgk1CEJIk6BrYozhc2fi7oUriD5/CePH98XadTNhbt4W27aexNKlu5GTU8B1zFaFGgQhpEkYt+xTAMDNPbtw/MSXcHXticjIJEya+ANu3IjnOF3rRA2CEMK5bkOc0WVQf4Ss/QX7d30MkUgffvO3YuPG4ygvr+A6XqtFDYIQwikVdXWMXfoJ0uLuYYAlYGtrjhEuX+D0aSnX0Vo9ahCEEE6N8JkF7bYGCN2wFif+mIfff79AzaGJ4Cty4S4uLoiLi0NCQgL8/f2rve7s7Iy8vDxIpVJIpVIsX74cAGBmZobz588jJiYG0dHRmDdvniJjEkI4IrK1Rv/JE3D94GF8tcgVBQUl+GTBb1zHIv/CFFF8Pp8lJiay9u3bMyUlJRYZGclsbW2rTOPs7MyCg4OrzWtsbMwcHBwYAKapqcni4+OrzVtTSSQShbwXKiqqhi8en8/8AnewFeeDmc/8sayCBbNp0wZznqu1VW2fmwrbg3B0dERiYiKSk5NRWlqKwMBAuLu712veR48eQSqVAgAKCwsRGxsLkUikqKiEEA708/SAeRdbhP22DSu/8sSZM1Ls3n2e61jkXxTWIEQiEVJTU+WPZTJZjR/yTk5OiIyMxIkTJ2BnZ1ftdQsLCzg4OODGjRs1rsfLywsSiQQSiQQGBgYN9wYIIQqjbdgWrr7eiLt6Hd4eXaCsLMSc2b9yHYv8h8IaBI/Hq/bcf7/9GBERAQsLC9jb22PDhg04cuRIldc1NDQQFBQEPz8/FBTU/AWZbdu2QSwWQywWIysrq8HyE0IUZ6y/HwRCIUpuncH48X3x1Zf7kZT0iOtY5D8U1iBkMhnMzc3lj83MzJCenl5lmoKCAhQVFQEAQkJCoKSkBH19fQCAUChEUFAQ9u7di8OHDysqJiGkkdkO6IvuwwcjdPcefPflJERGJuGnn45wHYvUQGENQiKRwMrKCpaWllBSUoKnpyeOHj1aZRojIyP5z2KxGHw+H9nZlbcH3L59O2JjY7F27VpFRSSENDJlNVWM/2whHiUmYYStCoyN22CW1y90v+gmTGGj466uriw+Pp4lJiayZcuWMQDM29ubeXt7MwDs448/ZtHR0SwyMpJdu3aNOTk5MQCsX79+jDHGbt++zaRSKZNKpczV1fWtRuOpqKi4L7cFc9maqGvM43+jWFn5X+ynn2Zynqm1V22fm7yXP7QIEokEYrGY6xiEkBqYWHfEggM7EXksBEvHiaCpqYquXT5GUdEzrqO1arV9btI3qQkhCsfj8TDhC3+U5BegM0+GLl2c4DbyS2oOTZxCv0lNCCEA0HuCOyy7d8Pt/XuweOFY7N9/CSEht7iORepADYIQolCa+rpw85uDxBs3Me/9d1BU9AwL/OhyGs0BNQhCiEK5L5oPZVVVqCaGYuDArli0cAcyM/O4jkXqgcYgCCEKY+3kiB5uLpD8vge/LnHHhQt3EBBwlutYpJ5oD4IQohBCFRV4fL4ITx48xPheOlBVVYb3rF+4jkVeAzUIQohCDPWaDoN2Zsi+cAQTJvTFyq8DkZiYwXUs8hqoQRBCGpxhewu8++EHiD51Cp/7DUNU1AP8+ONhrmOR10QNghDS4DyWL8aL4hKItbIhEunDa+YGlJaWcR2LvCZqEISQBiV2H4lO4h5IPnYQs2YOxcZfjiM8/B7XscgboAZBCGkwGm10MPpTX6RIb2P+5O5IS8vGZ5/t4ToWeUPUIAghDWbUpz5Q1dSEUWYEunWzxMdzN6GwsITrWOQNUYMghDSIDr0c4Dh2FO6d+AsLPnbBwYNXceyYhOtY5C1QgyCEvDWBkhImLF+MbFkaJvfXx7NnLzB/3lauY5G3RA2CEPLW3p0xBUYdLAHpGQxy7orFiwLw6FEu17HIW6IGQQh5KwbtzDB01v+QdPkiPvF2xuXL0di+/QzXsUgDoAZBCHkrHp8vQtmLUjgbF0JDQxXeszaCsRZzH7JWjRoEIeSNOYwcDmsnR+ReDsaE8X3w7f/9gfh4GdexSAOhBkEIeSNq2lpwXzwfj2Jj4ePZHXfvPsT33//JdSzSgOpsEG5ubuDxeI2RhRDSjIycPwcabXRgVRwDCwtDzPKiy2m0NHU2CE9PTyQkJOCHH36AjY1NY2QihDRxlt27oe+kcUi/GIIZH/THpl9P4Nq1OK5jkQZWZ4OYOnUqHBwccP/+fQQEBCAsLAxeXl7Q1NRsjHyEkCaGLxRgwgp/PH30CNPeNcajR3lYunQX17GIAtRrDKKgoABBQUEIDAyEiYkJxo0bh4iICPj4+Cg6HyGkiXGe6gkTq47QSbqK7u9YwtdnM/Lzi7mORRSgzgYxatQoHDp0COfPn4eSkhIcHR0xcuRIdO/eHQsXLmyMjISQJkLX1BjD58zE41thmD3VCYcOheHIketcxyIKUuc9qSdOnIi1a9fiypUrVZ4vKSnBhx9+qLBghJCmZ/yyhWCsHCPal+HFizLM893CdSSiQHXuQaxYsQLh4eHyx6qqqrCwsAAAnD9/XnHJCCFNSrehg2Dn3A8VkecwaKAdli7ZhfT0HK5jEQWqs0EcPHgQFRUV8sfl5eU4ePCgQkMRQpoWFQ11jFvyCfLuJ2DW+K4IDY3Bli0nuY5FFKzOBiEUClFaWip/XFpaCmVl5Xot3MXFBXFxcUhISIC/v3+1152dnZGXlwepVAqpVIrly5fXe15CSONx9fWGVlt92AuSoa2thllev9DlNFoJVludPn2ajR49Wv54zJgx7OzZs7XOA4Dx+XyWmJjI2rdvz5SUlFhkZCSztbWtMo2zszMLDg5+o3lrKolEUuc0VFRUr1dmdjZs9e1Q9tWOb1gFC2ZffjmZ80xUDVe1fW7WOUg9e/Zs7N27F7/88gt4PB5SU1Mxbdq0umaDo6MjEhMTkZycDAAIDAyEu7s7YmNjFTovIaTh8AWV33l4lpONqe+aIC5Ohm+//YPrWKSR1NkgkpKS4OTkBA0NDfB4PBQWFtZrwSKRCKmpqfLHMpkMvXv3rjadk5MTIiMjkZ6ejoULFyImJqbe8xJCFKufpwfM7WygF3cSloN6YeAAf7x4QZfTaC3qbBAAMHLkSHTp0gWqqqry51auXFnrPDVdv+m/xywjIiJgYWGBoqIiuLq64siRI7C2tq7XvH/z8vLCrFmzAAAGBgZ1vhdCSP3oGLXFCN9ZKIq9hXnjHbB1y0lcvRrDdSzSiOocpN60aRPee+89+Pr6gsfjYeLEifLTXGsjk8lgbm4uf2xmZob09PQq0xQUFKCoqAgAEBISAiUlJejr69dr3r9t27YNYrEYYrEYWVlZdeYihNTP2CWfQMDnY4wND5mZT+Hvv5PrSIQDtQ5g3L59u8qfGhoa7NSpU3UOfAgEAnb//n1maWkpH2i2s7OrMo2RkZH8Z7FYzFJSUuo9b01Fg9RUVA1Tds792Zqoa+zXAytZBQtmHh59Oc9EpZh6q0HqZ8+eAQCKi4thYmKC7OxstG/fvq7ZUF5eDh8fH5w6dQoCgQA7duxATEwMvL29AQBbtmzBhAkTMGfOHJSVlaGkpASenp61zksIUTxlNVWMW/YJnqUlYZabDY4evYGgoDCuYxGO1NpdPv/8c6ajo8PGjx/PMjIyWHp6Ovvqq68473o1Fe1BUFG9fY36xIetiQpjF66uYU/zDzAzMwPOM1Eprt54D4LH4+HcuXN4+vQpDh06hGPHjkFVVRX5+fm1zUYIaaZMrDth4NT3IIy/BmcPa/j6bIZMRmN7rVWtg9SMMaxZs0b++MWLF9QcCGmheHw+Jn7hD1b0FJ7Oxrh+PQ6bNoVwHYtwqM6zmE6fPo3x48c3RhZCCIf6THCHRfeusHkWgzY66pjl9UuV67CR1qfOQepPPvkEGhoaKCsrw7Nnz8Dj8cAYg46OTmPkI4Q0Ai19PbjNnwP+wyiMHmmLb//vD0RHp3Adi3Cszgahra3dGDkIIRxyXzwfamrKcBcJce9eGlauDOQ6EmkC6mwQAwYMqPH5/95AiBDSPHXu2xsOI4fDNC0Mlg7WeHfQUjx/Xlr3jKTFq7NBLFq0SP6zqqoqHB0dcevWLQwZMkShwQghiidUUcH4zxdCkJ0KjyEdsWP7aVy6FM11LNJE1NkgxowZU+WxmZkZVq1apbBAhJDGM2zW/9DWXIRB/DvIzlbHokUBXEciTUidZzH9l0wmQ9euXRWRhRDSiIw6WGLQjCnQS4/AO3Yi+M3fhtzc+l2tmbQOde5B/Pzzz/IrqfL5fNjb2+P27dsKD0YIURwejwePLxZDtawI4/sa4fhxCQ4coHFFUlWdDeLmzZvyn8vKyrB//36EhdF1WQhpzsTubujY0x49nksBtMXHczdxHYk0QXU2iD///BPPnj2Tf2GGz+dDTU0NJSUlCg9HCGl4GrptMOpTH7TJvod+zu2wwG8bHj58wnUs0gTVOQZx7tw5qKmpyR+rqanh7NmzCg1FCFGc0Z/6QkdbFaO6KEEiScCGDce4jkSaqDr3IFRVVeU39QGAoqIiqKurKzQUIUQxOop7QOw+Ep0LIqDbRoRhM3+my2mQV6pzD6KoqAgODg7yxz169KDDS4Q0QwIlJUxYvhhaRekY7tQOa348jDt3HnAdizRhde5B+Pn54eDBg/JbfpqYmOC9995TeDBCSMMa/NFUmHRoh6Fa95CY+BxffbWf60ikiavXWUw2Njbo3LkzeDwe4uLiUFZW1hjZCCENxMDCHEO9psMy/y4suxhj2NDP8ezZC65jkSauzkNMc+fOhYaGBu7evYvo6Ghoampizpw5jZGNENJAJny+GG34JXDtZYCdO8/h3Dn6LhOpW50NwsvLC0+fPpU/zsvLg5eXl0JDEUIaTo9RLrDu0xN9NdKQl1eERQt3cB2JNBN1HmLi8/nVHisrKyssECGk4ahpa2PMwnkwK0lGl66G+GDKj8jOprtCkvqps0GcOnUKf/zxBzZv3gzGGGbPno2QELoNISHNgduCOTAy0MQIy2KcPHkL+/Zd4joSaUbqbBD+/v6YNWsW5syZAx6PB6lUChMTk8bIRgh5C5b278BpgjscEAsetDF3Dl1Og7yeOscgGGO4fv06kpKS0KtXLwwZMgSxsbGNkY0Q8oYEQiEmfLEYJuWP4djFECu+2IsHDx5zHYs0M6/cg7CysoKnpyfef/99ZGdn48CBAwCAwYMHN1o4QsibcZ7+Piw7t8cQo4e4desR1q8/ynUk0gy9skHExcXhypUrGD16NO7fvw8AWLBgQaMFI4S8GT2RCYZ5fwi78kTo6uhghNcvKC+ny2mQ1/fKQ0weHh549OgRLly4gK1bt2Lw4MHg8XiNmY0Q8gbGf7YQIo0XGNRND+vW/gWp9D7XkUgzxmordXV1NnnyZBYcHMyKiorYr7/+yoYNG1brPFyVRCLhPAMVFZf1zvDBbF10GEtO+53dT/qNqaurcJ6JqmlXbZ+bdQ5SFxcXY9++fRg9ejTMzMwQGRmJJUuW1DUbIaSRqWpqYNySBbBBKixMdTBn9kYUFz/nOhZpxl7rntS5ubnYunUrhgwZUq/pXVxcEBcXh4SEBPj7+79yul69eqGsrAweHh7y5/z8/BAdHY2oqCjs27cPKioqrxOVkFbHdd5sWJhoY5C1Mn7//QJOn5ZyHYm0AArZbeHz+SwxMZG1b9+eKSkpscjISGZra1vjdOfOnWPHjx9nHh4eDAAzNTVlSUlJTFVVlQFgBw4cYNOnT3+rXSUqqpZc5l3t2OrbV1lM8h6W+WQvMzDQ5jwTVfOotzrE9KYcHR2RmJiI5ORklJaWIjAwEO7u7tWm8/X1RVBQEDIzM6s8LxQKoaamBoFAAHV1dfnlxgkhVfEFAkz8wh+dlXNgY9kGn37yG7Ky6HIa5O0prEGIRCKkpqbKH8tkMohEoirTmJqaYty4cdi8eXOV59PT0/Hjjz/i4cOHyMjIwNOnT3HmzJka1+Pl5QWJRAKJRAIDA4OGfyOENHH9J0+EdbeOGGT+AmfOSLFnzwWuI5EWQmENoqZTYhljVR6vW7cO/v7+1W552KZNG7i7u6N9+/YwNTWFhoYGpkyZUuN6tm3bBrFYDLFYjKysrIZ7A4Q0A22MjTDCxwu9lB9CwAfmzP6V60ikBanzWkxvSiaTwdzcXP7YzMys2mGiXr16ITAwEABgYGCAkSNHoqysDEpKSkhOTpZ/4B86dAh9+/bF3r17FRWXkGZp3NIFsNatQI8OWljivxNJSY+4jkRaGIUMfAgEAnb//n1maWkpH6S2s7N75fQBAQHyQWpHR0cWHR3N1NTUGAC2c+dO5uPj81aDLVRULa26vDuAbbgbyjJzDrAI6XomFAo4z0TV/Kq2z02F7UGUl5fDx8cHp06dgkAgwI4dOxATEwNvb28AwJYtW145b3h4OP78809ERESgrKwMUqkUW7duVVRUQpodZTU1jFv6CRxUM6CrrYKRXr+grKyc61ikBeK8gzVU0R4EVWup0Qt92f77l1kFC2Y//TST8zxUzbc42YMghCiGyMYagz6YiH6aGUhJKcTy5b9zHYm0UNQgCGlGeHw+JnzhDwedfJgZqsNt5CoUFT3jOhZpoRR2mishpOH1nTQO9r2s0VdUhv37LyEk5BbXkUgLRnsQhDQT2m0N4DrPG301M1BYUIYFfr9xHYm0cNQgCGkm3BfPR09TwMpEDR99uB6ZmXlcRyItHB1iIqQZsOnfB/1GDUa/tgW4cOEOAgLOch2JtALUIAhp4pRUVTD+s0Vw0s6EkA94z/qF60iklaAGQUgTN8z7Q4ht9dHNVIiVXwciMTGD60iklaAxCEKaMONOHTB8xvvor5uJqKgMrF59iOtIpBWhBkFIE8Xj8TBh+WI4tc2HrqYS3GZuoMtpkEZFh5gIaaIcx42CUz879DSuwMZfjiM8/B7XkUgrQ3sQhDRBmnq6GPPpx3DWe4K0tKf47LM9XEcirRA1CEKaoNELfdHfkg9TXWWMmbYJhYUlXEcirRAdYiKkibHq3QtDPYajd9tiHDx4FceOSbiORFopahCENCFCZWV4fL4IzvpZKC56hvnz6D4ohDt0iImQJmTwR1MxyN4QHfQLMctrBx49yuU6EmnFqEEQ0kS0tWyH0bM/QF/9PFy+HIft289wHYm0cnSIiZAmYsLyxXjXtBhKfAbvWRvBGOM6EmnlqEEQ0gT0HO2KYYO7wc6gAt/+3x+Ij5dxHYkQOsRECNfUdbThsdgHAw1ycPduOr7//k+uIxECgBoEIZwbteBjDLVWgp5mBUZ7bUBpaRnXkQgBQIeYCOFU+x7dMeYDVzgYPMOmX0/g2rU4riMRIkcNghCOCIRCTFrhj8GGeXj0KBdLl+7iOhIhVdAhJkI4Muh/UzDS0QjGWsUYP20T8vOLuY5ESBXUIAjhgL6ZCBPnTUdvgwIcOnQdR45c5zoSIdXQISZCODD+s4UY1q4EJcXPMM93C9dxCKkR7UEQ0sjsXYZg/Ch7tNcpxNw5O5GensN1JEJqpNA9CBcXF8TFxSEhIQH+/v6vnK5Xr14oKyuDh4eH/DkdHR0cPHgQsbGxiImJQZ8+fRQZlZBGoaqlifeX+6F/23yEhsZiy5aTXEcipFZMEcXn81liYiJr3749U1JSYpGRkczW1rbG6c6dO8eOHz/OPDw85M/v3LmTffTRRwwAU1JSYjo6OnWuUyKRKOS9UFE1VI3/bCGLzjnFnr04zGxtzTnPQ0VV2+emwvYgHB0dkZiYiOTkZJSWliIwMBDu7u7VpvP19UVQUBAyMzPlz2lpaWHgwIHYvn07AKC0tBRPnz5VVFRCGkW7bnbw/NANdrov8P23BxEbm8p1JEJqpbAGIRKJkJr6zz8AmUwGkUhUZRpTU1OMGzcOmzdvrvJ8hw4d8OTJEwQEBCAiIgLbtm2Durq6oqISonB8gQCTv/LHEJMCxMen4dtv/+A6EiF1UliD4PF41Z7779Up161bB39/f1RUVFR5XigUokePHti0aRN69OiBoqIiLFmypMb1eHl5QSKRQCKRwMDAoOHeACENaMCUSRg/QARdNR68Zv6MFy/ochqkeVDIca0+ffqwkydPyh8vWbKELVmypMo0SUlJLDk5mSUnJ7OCggL2+PFj5u7uzoyMjFhycrJ8uv79+7Njx4691bE0KiquStfEmG2POsPKyo+yzZs/5jwPFdW/q7bPTYWd5iqRSGBlZQVLS0ukpaXB09MTkydPrjJNhw4d5D8HBATg2LFj+OuvvwAAqampsLa2xr179zBkyBDExMQoKiohDUKoogJ9M1Pom4mgby6CgbkIemamMLexhmv7F8h88hT+/ju5jklIvSmsQZSXl8PHxwenTp2CQCDAjh07EBMTA29vbwDAli21fznI19cXe/fuhbKyMpKSkjBjxgxFRSWk3jT1dKFnZgoDcxH0zc3+aQhmIugYtf3XlAzKLwqgUpSFTjo5MNbQx8Tpm/D0aRFn2Ql5XTxU7kq0CBKJBGKxmOsYpBnjCwXQNTaGvrlIvidQ+bMp9M1FUNXQqDJ9fmYmeHmPoVGWD13lUhhrC9HOWAsdLA2grq4in2737vP43/S1jf12CKlTbZ+b9E1q0uqoaKj/5zCQ6OUegQhtjI0gEP7zz6L0+XNky9KRn56B0tR7aCN4DkMtAcwN1dHR0gBW/U2grGwCwAQA8PDhE8TEPMTZUzcRE/MQMTGpiI1NRW5uIUfvlpA3Rw2CtDg8Hg9abQ0qP/TNTCsPBZmLoC+q3AvQ1NOtMn1Rbh6yUtOQcucu7p45By0Uo606IDJQhXU7fdjamaOjW0cIhdYAgIqKCiQlPUJsrAzHj4UjJiYVMTGpiIuTobCwhIu3TIhCUIMgzZJQWRl6IpN/DgP9+3CQyBRKqv8c3qkoL0duxiNky9IRde4SslNleJ6TBT3VcpjqqaBTe0MMszOHnVM7WFjYg8+vPPu7tLQMiYkZiI5+iIN/XJXvDcTHp+HZsxdcvXVCGg01CNJkqeto/zMQbC6CgbmZfIBY27Ct/IMcAJ4XFyM7NQ2ZySmIu3INWakyZKemgRU9hameMjpbi2BnZw7b/uawtR0KM7N/vjPz7NkLxMen4fr1OOwMOCs/NJSYmEG3/yStGjUIwhm+QIA2xob/2gt4eTjIrPJnNW2tKtM/zXyCHFk6Em7cQrYsrbJSK0tTGbCzM4edXTv0tjOHzSgx7OzGw9CwjXz+wsISxMbKcP78HcTGpMobQXLy42pf1iSEUIMgCqaspgZ9c1Pom/17T6ByYFjP1AQCpX9+BctKS5EjS0e2LA0Pbke9/PCXIfvlc2XPX8Dc3AB2du3Qzc4ctu7WsLUbAjs7c7RpoylfTl5eIWJiUhF8NBwxMQ8RGytDTMxDpKZmVfs2PyHk1ahBkLemZaBf7cthBi8HhrX09apMW/w0H9myNKTFxuP26fPIkaUh6+VewNPMJ2AVFeDz+ejQwQh2du3g1NsctjPEsLMzh42NGTQ11eTLyszMQ0xMKvbvu4zY2H/2CB49ym3sTUBIi0QNgtSbQEkJlvbdYNWnF0ysOkLfTAQ9kSlU1P/50K6oqEDeo8fITk1DzMWrlR/+fx8KkqWhJL9APq2SkhCdOpnA3s4cdiPeha2dOWxtzdG5swiqqsry6WSyLMTEpGLH9jMvzxiq3CvIzs5v1PdPSGtDDYK8Eo/Hg4l1J1j3EcPaSYz2PeyhrKaK8rIyZCanIFuWhnvXwisPAaXKkJWahtz0RygvLa2yHFVVZXTuLMIAtx6wtTWDrV072NmZo1MnEyj96xBTcvJjxMQ8xJnTUnkTiI1NRX5+cWO/dUIIqEGQ/2hjbARrJ0dY9+mFTr17yQ8RPUpMwvWgv3DvmgRJt6R4XlT9Q1tTUw097C1ha2teecbQy0bQvr2R/Iyj8vJyJCZmICYmFYcPXXt5aCgV8fEyFBc/b9T3SgipHTWIVk5VSxOdxD1h7SSGdR8x2lq2A1B5xlB86A3cuy5BwnUJ8p9kyefR1lZHz7628rOG/j401K7dP9cievGiFPHxabh16z5+33NBfmgoISGdLnVNSDNBDaKVESgpwbJ7V1g5iWHdWwzzrrbgCwR4XlyM+xIpQgODcO+6BI/vJ8vn6dTJBGNdB6NvX1s49bVBly7t5HsExcXPERcnw+XL0S9PHa38Mtn9+xkoL6dTRwlpzqhBtAIm1h1h1UcMaydHdOhhDxV1NZSXleFhVAzObt2Je9cleHjnLsrLyqCqqoyePTth2vjx6NvPFn372qJtWx0AlaePXrsWj4N/XIVUmoS7dx8iJSWTTh0lpIWiBtECtTEyrNxD6COGVR+xfBzhcdIDhB8ORsJ1Ce7flOJZYRFMTPTQt68NfKZMh1NfG/To0RHKykoAgHv30nD8+E1cC4tFWFgsYmJSqRkQ0opQg2gBVDU10FHc4+XgshiG7S0AAPlZ2bh3LRwJ1yW4d12CwqxsdOtmCee+Nlgyczb69rVF+/ZGAICSkueQSBKw9qe/EBYWi2vX4pCVRaeREtKaUYNohgRCISy6d3152EiMdl3t/hlHuCnFtYNHcO+6BCWZj9CnT2eM7mcLJ9/56N3bWv5Fs/T0bISGxmLDz8EIC4uFVJpE1x0ihFRBDaKZMO7UAdZOjrDq0wsdezlARV0dFeXleBgdg3O/7ca9a+FQLs5Bb0crTOlvA6fFC9C1a+WeRHl5OW7ffoBdO88hLCwOoaExePjwCcfviBDS1FGDaKK0DdvKv6Bm1bsXtNtWXn00MzkFkr9O4GGEFDoVT9HL3hLeQ23Q94uF8sHk3NxCXLsWhwOBlxEWFofw8HsoKnrG5dshhDRD1CCaCBUNdXQS94DVy4Fl447tAQAF2TlIuHETuXF3YcAvhH0XU4wdY4MeX/aXfws5Pl6GY8ckLweT4xAbS4PJhJC3Rw2CI3yhABbdushPP23XzQ4CoRAvSp4hOUKK3FtX0VZYjK6dDDBtkg0sLXsA+Gcwec2PhxEWFovr1+NpMJkQohDUIBqRUcf28lNPO4odoKqhgYrycmTdi0f+tVMwUi5B9/Z6+HiKFTQ1rQAAaWmVg8nr1x1FWFgsIiOTaTCZENIoqEEokHZbA1j17iUfXNYxbAuAoSIrDSoyKUzUXsC2vR7sJpoBMERZWTlu307GzoCzCA2tPFyUmkqDyYQQblCDaEAq6uro0MtBPrhs3KkDhDwG7bJcqOXJYFLyEHYd9aDfVQuADXJzCxEWFot9v19AWFgsJJIEGkwmhDQZ1CDeAl8oQLsudpVnGvURw+KdrtBW48FIqQSaRY9gqhaPTu3aQElJAMAUcXEyHD1yHWFhcQgLi0VcnIwGkwkhTRY1iNdk2N7i5ZVPHdFJ7ABzA1UYq72AblkOzLTSYainDgAoLlZDePg9HDlQ+d2D69fj6QY3hJBmhRpEHbT09eTXNerWXwwbizYwVS+FAb8QIu0iqCpXHhKSyQpx4Uys/FTTyMgklJWVc5yeEELeHDWI/1BWU0OHXvaw7iNGv2F98I6tCUzVS2Gs8hxtNQAgH2Vl5YiMTMKJl99KDguLg0yWVdeiCSGkWWn1DYLH58O8qy26DXTC4JFO6NHdHGaaDCbqpVAXMgCFyHtajNALdxH28qqmEkkC3f2MENIqMEWVi4sLi4uLYwkJCczf3/+V0/Xq1YuVlZUxDw+PKs/z+XwWERHBgoOD67U+iUTy2hnVNNSYrOA4K60IZhWsshKSd7DtAX7sww+HMRsbM8bj8RS2jaioqKi4rNo+NxW2B8Hn87Fx40YMGzYMMpkMEokER48eRWxsbLXpfvjhB5w6daraMubPn4/Y2Fhoa2srKiZKikoQfjkCCbEpuHQ+EtevxyMnp0Bh6yOEkOaCr6gFOzo6IjExEcnJySgtLUVgYCDc3d2rTefr64ugoCBkZmZWeV4kEsHNzQ2//faboiLKjXdbDv+Fv+HEiZvUHAgh5CWFNQiRSITU1FT5Y5lMBpFIVGUaU1NTjBs3Dps3b642/7p167B48WJUVNR+X2MvLy9IJBJIJBIYGBg0THhCCCGKaxA8Hq/ac//9Uti6devg7+9frQm4ubkhMzMTERERda5n27ZtEIvFEIvFyMqiM4kIIaShKGwMQiaTwdzcXP7YzMwM6enpVabp1asXAgMDAQAGBgYYOXIkysrK0Lt3b4wZMwYjR46EqqoqtLW1sWfPHkydOlVRcQkhhNRAISPjAoGA3b9/n1laWjIlJSUWGRnJ7OzsXjl9QEBAtbOYADBnZ2eFnsVERUVF1ZqLk7OYysvL4ePjg1OnTkEgEGDHjh2IiYmBt7c3AGDLli2KWjUhhJAGwENlp2gRJBIJxGIx1zEIIaTZqO1zU2GD1IQQQpo3ahCEEEJq1KIOMWVmZiIlJeWN5jUwMGiSp8lSrtdDuV4P5Xo9LTGXhYUFDA0NX/k656PoTaGa6hlQlItyUa6mU60tFx1iIoQQUiNqEIQQQmpEDeKlrVu3ch2hRpTr9VCu10O5Xk9ry9WiBqkJIYQ0HNqDIIQQUiNqEIQQQmrUqhrE9u3b8fjxY0RFRb1ymvXr1yMhIQG3b9+Gg4NDk8jl7OyMvLw8SKVSSKVSLF++vFFymZmZ4fz584iJiUF0dDTmzZtX43SNvc3qk4uLbaaiooIbN24gMjIS0dHR+PLLL2ucrrG3V31ycfU7BlTeVTIiIgLBwcE1vs7Fv8m6cnG1vZKTk3Hnzh1IpVJIJJIap2no7cX5ObyNVQMGDGAODg4sKiqqxtddXV3ZiRMnGADWu3dvdv369SaR63WuaNuQZWxszBwcHBgApqmpyeLj45mtrS3n26w+ubjaZhoaGgwAEwqF7Pr166x3796cb6/65OJqewFgCxYsYHv37q1x/Vxtr7pycbW9kpOTmb6+/itfb+jt1ar2IK5cuYKcnJxXvu7u7o7du3cDAG7cuIE2bdrA2NiY81xcefToEaRSKQCgsLAQsbGx1e4KyMU2q08urhQVFQEAlJSUoKSkVO0mWVz9jtWViyt13VqYq+3VmLc8bkgNvb1aVYOoS31uk8oVJycnREZG4sSJE7Czs2v09VtYWMDBwQE3btyo8jzX2+xVuQButhmfz4dUKkVmZibOnDmD8PDwKq9ztb3qygVws73qurUwV9urPrc85mJ7McZw+vRp3Lx5E15eXtVeb+jtRQ3iX+pzm1QuREREwMLCAvb29tiwYQOOHDnSqOvX0NBAUFAQ/Pz8UFBQUOU1LrdZbbm42mYVFRVwcHCAmZkZHB0d0aVLlyqvc7W96srFxfaqz62Fudhe9cnF1e9Xv3790LNnT7i6uuLjjz/GgAEDqrze0NuLGsS/1Oc2qVwoKCiQHyIICQmBkpIS9PX1G2XdQqEQQUFB2Lt3Lw4fPlztda62WV25uNxmAPD06VNcvHgRI0aMqPI8179jr8rFxfbq168fxowZg+TkZAQGBmLw4MHYs2dPlWm42F71ycXV71dGRgYA4MmTJzh8+DAcHR2rvK6I7cXJwBRXZWFh8crB4JEjR1YZ4Llx40aTyGVkZCT/WSwWs5SUlEbLtWvXLrZ27dpXvs7VNqsrFxfbzMDAgOno6DAATFVVlV2+fJm5ublxvr3qk4vL3zHg1YO+XP6brC0XF9tLXV2daWpqyn8ODQ1lLi4uCt1eCrvlaFO0b98+DBo0CAYGBkhNTcWKFSugpKQEoPIWqCdOnMDIkSORmJiI4uJizJgxo0nkmjBhAubMmYOysjKUlJTA09OzUXL169cP06ZNk59WBwDLli1Du3bt5Nm42Gb1ycXFNjMxMcGuXbsgEAjA5/Pxxx9/4Pjx41Vus8vF9qpPLq5+x2rC9faqTy4utpeRkZF8b1koFGLfvn04deqUQrcXXWqDEEJIjWgMghBCSI2oQRBCCKkRNQhCCCE1ogZBCCGkRtQgCCGE1IgaBCEv/f1tbAsLC7z//vsNuuylS5dWeRwaGtqgyydEURr1iydUVE21CgoKGPBmV+rk8/n1WjYVVXMq2oMg5D++//57DBgwAFKpFH5+fuDz+Vi1ahXCw8Nx+/ZtzJo1C0DlPQHOnz+PvXv3yu/lcfjwYdy8eRPR0dHyi6l99913UFNTg1Qqxe+//w4AVa4dtWrVKkRFReHOnTuYNGmSfNkXLlzAwYMHERsbK5/v7+XdvXsXt2/fxurVqxtlm5DWi/MuRUXVFOpVexBeXl7ss88+YwCYsrIyk0gkzNLSkjk7O7PCwkJmaWkpn1ZXV5cBlZe0iIqKYnp6elWW/d91jR8/np0+fZrx+XxmaGjIUlJSmLGxMXN2dmZ5eXlMJBIxHo/HwsLCWL9+/Ziuri6Li4uTL+fvS2hQUSmiaA+CkDoMHz4c06ZNg1QqxY0bN6Cvrw8rKysAQHh4OB48eCCfdt68eYiMjMT169dhbm4un+5V+vfvj/3796OiogKZmZm4dOkSxGKxfNlpaWlgjCEyMhKWlpbIz8/Hs2fP8Ntvv2HcuHEoLi5W2PsmhBoEIXXg8Xjw9fWFg4MDHBwc0KFDB5w5cwbAPzfiASoPCw0dOhROTk6wt7eHVCqFqqpqnct+lefPn8t/Li8vh1AoRHl5ORwdHREUFISxY8fi5MmTb/nuCHk1ahCE/EdBQQG0tLTkj0+dOoU5c+ZAKKy8tqWVlRXU1dWrzaejo4Pc3FyUlJSgc+fO6NOnj/y10tJS+fz/dvnyZbz33nvg8/kwMDDAwIEDa7yZz980NDSgo6ODkJAQ+Pn5wd7e/i3eKSG1a1VXcyWkPu7cuYOysjJERkZi586dWL9+PSwtLREREQEej4cnT55g7Nix1eY7efIkZs+ejdu3byM+Ph7Xr1+Xv7Z161bcuXMHERER+OCDD+TPHz58GE5OTrh9+zYYY1i8eDEeP34MGxubGrNpaWnhr7/+gqqqKng8HhYsWNDg75+Qv9HVXAkhhNSIDjERQgipETUIQgghNaIGQQghpEbUIAghhNSIGgQhhJAaUYMghBBSI2oQhBBCavT/qGaBjZRa04sAAAAASUVORK5CYII=",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-03-08T10:31:50.691508</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m9b4ee68692\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(57.410369 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"103.407386\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(95.455824 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"141.452841\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(133.501278 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"179.498295\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(171.546733 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"217.54375\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 3.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(209.592188 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"255.589205\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 3.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(247.637642 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"293.634659\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 4.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(285.683097 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"331.680114\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 4.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(323.728551 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m9b4ee68692\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 5.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(361.774006 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Iterations -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(193.730469 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-49\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4ef2444fe2\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"231.953809\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.44 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 235.753028)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"195.483897\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.46 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 199.283115)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"159.013984\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.48 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 162.813203)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"122.544071\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.50 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 126.34329)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"86.074158\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.52 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 89.873377)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ef2444fe2\" y=\"49.604246\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.54 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(20.878125 53.403464)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Accuracy -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(14.798438 153.86625)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 1331 1722 \nL 3047 1722 \nL 2188 4044 \nz\nM 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 3669 0 \nL 3244 1197 \nL 1141 1197 \nL 716 0 \nL 50 0 \nL 1831 4666 \nz\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-41\"/>\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"176.619141\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"239.998047\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"281.111328\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"342.390625\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"397.371094\" xlink:href=\"#DejaVuSans-79\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p76b77023a3)\" d=\"M 65.361932 229.436888 \nL 141.452841 209.112695 \nL 217.54375 198.809262 \nL 293.634659 76.440095 \nL 369.725568 32.201761 \n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p76b77023a3)\" d=\"M 65.361932 229.874489 \nL 141.452841 212.590795 \nL 217.54375 200.188755 \nL 293.634659 80.39033 \nL 369.725568 33.94865 \n\" style=\"fill:none;stroke:#feffb3;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Accuracy Curve -->\n    <g style=\"fill:#ffffff;\" transform=\"translate(170.54125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" id=\"DejaVuSans-43\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-41\"/>\n     <use x=\"66.658203\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"121.638672\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"176.619141\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"239.998047\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"281.111328\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"342.390625\" xlink:href=\"#DejaVuSans-63\"/>\n     <use x=\"397.371094\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"456.550781\" xlink:href=\"#DejaVuSans-20\"/>\n     <use x=\"488.337891\" xlink:href=\"#DejaVuSans-43\"/>\n     <use x=\"558.162109\" xlink:href=\"#DejaVuSans-75\"/>\n     <use x=\"621.541016\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"662.654297\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"721.833984\" xlink:href=\"#DejaVuSans-65\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.14375 59.674375 \nL 138.525 59.674375 \nQ 140.525 59.674375 140.525 57.674375 \nL 140.525 29.318125 \nQ 140.525 27.318125 138.525 27.318125 \nL 57.14375 27.318125 \nQ 55.14375 27.318125 55.14375 29.318125 \nL 55.14375 57.674375 \nQ 55.14375 59.674375 57.14375 59.674375 \nz\n\" style=\"opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 59.14375 35.416562 \nL 79.14375 35.416562 \n\" style=\"fill:none;stroke:#8dd3c7;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_19\">\n     <!-- Train -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 59.14375 50.094687 \nL 79.14375 50.094687 \n\" style=\"fill:none;stroke:#feffb3;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_20\">\n     <!-- Validation -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1831 0 \nL 50 4666 \nL 709 4666 \nL 2188 738 \nL 3669 4666 \nL 4325 4666 \nL 2547 0 \nL 1831 0 \nz\n\" id=\"DejaVuSans-56\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-56\"/>\n      <use x=\"60.658203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"121.9375\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"149.720703\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"177.503906\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"240.980469\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"302.259766\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"341.46875\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"369.251953\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"430.433594\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p76b77023a3\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final round Traing Accuracy: 0.5495434745539383\n",
      "Final round Validation Accuracy: 0.5485854858548586\n"
     ]
    }
   ],
   "source": [
    "''' Answer: Train '''\n",
    "model = AutoEncoder()\n",
    "train(model) # use default hyperparameters\n",
    "# num_epochs = 5, learning_rate = 1e-4, batch_size = 64\n",
    "\n",
    "#Final round Traing Loss: 0.05527644604444504 \n",
    "#Final round Validation Loss: 0.06122467294335365\n",
    "#Final round Traing Accuracy: 0.5358089568897363\n",
    "#Final round Validation Accuracy: 0.5322335576296939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9xTwIf51prF"
   },
   "source": [
    "### Part (d) [5 pt]\n",
    "\n",
    "Tune your hyperparameters, training at least 4 different models (4 sets of hyperparameters).\n",
    "\n",
    "Do not include all your training curves. Instead, explain what hyperparameters\n",
    "you tried, what their effect was, and what your thought process was as you \n",
    "chose the next set of hyperparameters to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-kiD7vxoubbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Answer: Tune the hyperparameters'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: Tune the hyperparameters'''\n",
    "# The hyperparameters in this case are ① Number of epoches ② Batch size, and ③ Learning rate\n",
    "# I will investigate each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "PhTKt9iL1prG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0af60546-eb00-46f0-9169-49ab97929ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch       train_loss       val_loss       train_acc       val_acc\n",
      "____1____       ___0.07___       __0.07__       __0.45___       _0.45__\n",
      "____2____       ___0.07___       __0.07__       __0.46___       _0.46__\n",
      "____3____       ___0.07___       __0.07__       __0.46___       _0.46__\n",
      "____4____       ___0.06___       __0.06__       __0.46___       _0.46__\n",
      "____5____       ___0.06___       __0.06__       __0.54___       _0.54__\n",
      "____6____       ___0.05___       __0.05__       __0.56___       _0.56__\n",
      "____7____       ___0.06___       __0.06__       __0.57___       _0.57__\n",
      "____8____       ___0.05___       __0.05__       __0.57___       _0.57__\n",
      "____9____       ___0.05___       __0.05__       __0.57___       _0.57__\n",
      "___10____       ___0.04___       __0.04__       __0.57___       _0.57__\n",
      "___11____       ___0.04___       __0.05__       __0.57___       _0.57__\n",
      "___12____       ___0.05___       __0.05__       __0.57___       _0.57__\n",
      "___13____       ___0.04___       __0.05__       __0.57___       _0.57__\n",
      "___14____       ___0.04___       __0.04__       __0.57___       _0.57__\n",
      "___15____       ___0.04___       __0.04__       __0.58___       _0.57__\n",
      "___16____       ___0.04___       __0.04__       __0.58___       _0.58__\n",
      "___17____       ___0.05___       __0.04__       __0.58___       _0.58__\n",
      "___18____       ___0.05___       __0.04__       __0.58___       _0.58__\n",
      "___19____       ___0.04___       __0.04__       __0.58___       _0.58__\n",
      "___20____       ___0.04___       __0.04__       __0.58___       _0.58__\n",
      "--------------------------------------------------------------\n",
      "Final round Traing Loss: 0.040230728685855865\n",
      "Final round Validation Loss: 0.03923390433192253\n",
      "Final round Traing Accuracy: 0.5786014354586182\n",
      "Final round Validation Accuracy: 0.5780696042254541\n"
     ]
    }
   ],
   "source": [
    "''' Answer: 1. Increase number of epoches''' \n",
    "model = AutoEncoder()\n",
    "train(model, num_epochs=20, plot=False)\n",
    "\n",
    "#Final round Traing Loss: 0.033555589616298676\n",
    "#Final round Validation Loss: 0.0363919697701931\n",
    "#Final round Traing Accuracy: 0.5904369932877583\n",
    "#Final round Validation Accuracy: 0.590044135735475\n",
    "\n",
    "# Since this model works much better than only 5 epoches, later hyperparameters search will use num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "id": "kge9Wbekubbh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b1a4b835-50b0-48bf-81e0-c700cd3b73a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch       train_loss       val_loss       train_acc       val_acc\n",
      "____1____       ___0.06___       __0.05__       __0.57___       _0.57__\n",
      "____2____       ___0.04___       __0.04__       __0.58___       _0.58__\n",
      "____3____       ___0.04___       __0.04__       __0.60___       _0.60__\n",
      "____4____       ___0.03___       __0.03__       __0.60___       _0.60__\n",
      "____5____       ___0.03___       __0.03__       __0.60___       _0.60__\n",
      "____6____       ___0.03___       __0.03__       __0.61___       _0.61__\n",
      "____7____       ___0.03___       __0.04__       __0.60___       _0.60__\n",
      "____8____       ___0.02___       __0.03__       __0.61___       _0.61__\n",
      "____9____       ___0.03___       __0.03__       __0.61___       _0.61__\n",
      "___10____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "___11____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "___12____       ___0.03___       __0.02__       __0.61___       _0.61__\n",
      "___13____       ___0.02___       __0.02__       __0.62___       _0.61__\n",
      "___14____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "___15____       ___0.01___       __0.01__       __0.62___       _0.61__\n",
      "___16____       ___0.02___       __0.01__       __0.62___       _0.62__\n",
      "___17____       ___0.02___       __0.02__       __0.63___       _0.62__\n",
      "___18____       ___0.02___       __0.02__       __0.62___       _0.62__\n",
      "___19____       ___0.01___       __0.01__       __0.63___       _0.62__\n",
      "___20____       ___0.01___       __0.02__       __0.63___       _0.63__\n",
      "--------------------------------------------------------------\n",
      "Final round Traing Loss: 0.012691423296928406\n",
      "Final round Validation Loss: 0.016883984208106995\n",
      "Final round Traing Accuracy: 0.6307181943604768\n",
      "Final round Validation Accuracy: 0.6252803704507633\n"
     ]
    }
   ],
   "source": [
    "''' Answer: 2. Increase training rate'''\n",
    "model = AutoEncoder()\n",
    "train(model, num_epochs=20, learning_rate=1e-3, plot=False)\n",
    "\n",
    "#Final round Traing Loss: 0.014145386405289173\n",
    "#Final round Validation Loss: 0.01599755510687828\n",
    "#Final round Traing Accuracy: 0.6250213148552916\n",
    "#Final round Validation Accuracy: 0.6219521018739599\n",
    "\n",
    "# Since this model with lr=1e-3 work much better than lr=1e-4, later hyperparameters search will use lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "id": "OKcql28Lubbi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "30b5d3a3-5a19-4d06-ff60-e1e7b4ac9bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch       train_loss       val_loss       train_acc       val_acc\n",
      "____1____       ___0.06___       __0.06__       __0.48___       _0.48__\n",
      "____2____       ___0.05___       __0.06__       __0.57___       _0.57__\n",
      "____3____       ___0.05___       __0.05__       __0.58___       _0.58__\n",
      "____4____       ___0.04___       __0.04__       __0.58___       _0.58__\n",
      "____5____       ___0.04___       __0.04__       __0.59___       _0.59__\n",
      "____6____       ___0.03___       __0.04__       __0.60___       _0.60__\n",
      "____7____       ___0.03___       __0.04__       __0.61___       _0.61__\n",
      "____8____       ___0.03___       __0.03__       __0.61___       _0.61__\n",
      "____9____       ___0.03___       __0.03__       __0.62___       _0.62__\n",
      "___10____       ___0.03___       __0.03__       __0.62___       _0.62__\n",
      "___11____       ___0.02___       __0.03__       __0.62___       _0.62__\n",
      "___12____       ___0.03___       __0.03__       __0.63___       _0.62__\n",
      "___13____       ___0.02___       __0.03__       __0.62___       _0.62__\n",
      "___14____       ___0.02___       __0.03__       __0.62___       _0.62__\n",
      "___15____       ___0.02___       __0.03__       __0.62___       _0.62__\n",
      "___16____       ___0.02___       __0.02__       __0.63___       _0.62__\n",
      "___17____       ___0.02___       __0.02__       __0.62___       _0.61__\n",
      "___18____       ___0.03___       __0.02__       __0.62___       _0.62__\n",
      "___19____       ___0.02___       __0.02__       __0.62___       _0.62__\n",
      "___20____       ___0.02___       __0.02__       __0.63___       _0.62__\n",
      "--------------------------------------------------------------\n",
      "Final round Traing Loss: 0.015217630192637444\n",
      "Final round Validation Loss: 0.019067775458097458\n",
      "Final round Traing Accuracy: 0.6264087181633571\n",
      "Final round Validation Accuracy: 0.6223500470298821\n"
     ]
    }
   ],
   "source": [
    "''' Answer: 3. Increase batch size'''\n",
    "model = AutoEncoder()\n",
    "train(model, num_epochs=20, batch_size=128, learning_rate=1e-3, plot=False)\n",
    "\n",
    "#Final round Traing Loss: 0.016742540523409843\n",
    "#Final round Validation Loss: 0.022027965635061264\n",
    "#Final round Traing Accuracy: 0.6142863786448403\n",
    "#Final round Validation Accuracy: 0.6120396498082628\n",
    "\n",
    "# Since this model with bs=128 did outperform than the previous set of parameters with bs=64, we discard this set of parameters. Later I will discover whether decrease the batch size will lead to better accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "id": "I_IZE1rKubbi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6565b06e-39d2-43db-a18a-430c7e6d32c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epoch       train_loss       val_loss       train_acc       val_acc\n",
      "____1____       ___0.05___       __0.04__       __0.58___       _0.58__\n",
      "____2____       ___0.03___       __0.04__       __0.60___       _0.60__\n",
      "____3____       ___0.03___       __0.03__       __0.60___       _0.60__\n",
      "____4____       ___0.02___       __0.02__       __0.59___       _0.59__\n",
      "____5____       ___0.02___       __0.03__       __0.61___       _0.61__\n",
      "____6____       ___0.02___       __0.03__       __0.61___       _0.61__\n",
      "____7____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "____8____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "____9____       ___0.02___       __0.01__       __0.61___       _0.61__\n",
      "___10____       ___0.02___       __0.02__       __0.61___       _0.61__\n",
      "___11____       ___0.01___       __0.03__       __0.62___       _0.62__\n",
      "___12____       ___0.02___       __0.02__       __0.62___       _0.62__\n",
      "___13____       ___0.02___       __0.01__       __0.63___       _0.62__\n",
      "___14____       ___0.01___       __0.02__       __0.63___       _0.62__\n",
      "___15____       ___0.01___       __0.02__       __0.63___       _0.63__\n",
      "___16____       ___0.01___       __0.01__       __0.63___       _0.63__\n",
      "___17____       ___0.02___       __0.01__       __0.63___       _0.63__\n",
      "___18____       ___0.02___       __0.01__       __0.64___       _0.63__\n",
      "___19____       ___0.01___       __0.01__       __0.62___       _0.61__\n",
      "___20____       ___0.02___       __0.02__       __0.63___       _0.62__\n",
      "--------------------------------------------------------------\n",
      "Final round Traing Loss: 0.018925685435533524\n",
      "Final round Validation Loss: 0.01778177171945572\n",
      "Final round Traing Accuracy: 0.6309042149157482\n",
      "Final round Validation Accuracy: 0.6242674191447797\n"
     ]
    }
   ],
   "source": [
    "''' Answer: 4. Decrease batch size'''\n",
    "model = AutoEncoder()\n",
    "train(model, num_epochs=20, batch_size=32, learning_rate=1e-3, plot=False)\n",
    "\n",
    "#Final round Traing Loss: 0.017107781022787094\n",
    "#Final round Validation Loss: 0.009987073950469494\n",
    "#Final round Traing Accuracy: 0.6558387201785797\n",
    "#Final round Validation Accuracy: 0.6512553360827726\n",
    "\n",
    "# Best accuracy and loss found so far with hyperparameters ① num_epochs=20, ② batch_size=32 and ③ learning_rate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4I2GQzTUubbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Answer: Effect of each hyperparameters'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Answer: Effect of each hyperparameters'''\n",
    "# Increased number of epochs simply train the network for a longer time, thus will result in better performance given the model is not overfitting.\n",
    "# Increasing learning rate make faster convergence to better solution . In my tuning above, lr=1e-3 is a moderate learning rate.\n",
    "# Larger batch size allow more training data to be assessed every iteration and and smaller batch size will update the parameters more frequently. In my tuning above, bs=32 is a moderate batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymCsZH291prI"
   },
   "source": [
    "# Part 4. Testing [12 pt]\n",
    "\n",
    "### Part (a) [2 pt]\n",
    "\n",
    "Compute and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "id": "0OkSbup91prJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7e08c53b-73a5-438c-a315-0c7b4dc62833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 63.11%\n"
     ]
    }
   ],
   "source": [
    "''' Answer: Test Accuracy'''\n",
    "# I found the best set of hyperparemters from the 4th try from last part, I will use this model for the test accuracy\n",
    "\n",
    "model_path = \"./data/Lab4/\" + \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(\"AutoEncoder\", 32, 0.001, 20)\n",
    "state = torch.load(model_path) # load from the best model found so far\n",
    "\n",
    "encoder = AutoEncoder()\n",
    "encoder.load_state_dict(state)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "test_acc = round(get_accuracy(encoder, test_loader) * 100, 2)\n",
    "\n",
    "print(\"Test accuracy is {}%\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEe9yt6L1prM"
   },
   "source": [
    "### Part (b) [4 pt]\n",
    "\n",
    "Based on the test accuracy alone, it is difficult to assess whether our model\n",
    "is actually performing well. We don't know whether a high accuracy is due to\n",
    "the simplicity of the problem, or if a poor accuracy is a result of the inherent\n",
    "difficulty of the problem.\n",
    "\n",
    "It is therefore very important to be able to compare our model to at least one\n",
    "alternative. In particular, we consider a simple **baseline**\n",
    "model that is not very computationally expensive. Our neural network\n",
    "should at least outperform this baseline model. If our network is not much\n",
    "better than the baseline, then it is not doing well.\n",
    "\n",
    "For our data imputation problem, consider the following baseline model:\n",
    "to predict a missing feature, the baseline model will look at the **most common value** of the feature in the training set. \n",
    "\n",
    "For example, if the feature \"marriage\" is missing, then this model's prediction will be the most common value for \"marriage\" in the training set, which happens to be \"Married-civ-spouse\".\n",
    "\n",
    "What would be the test accuracy of this baseline model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "id": "p45VHp011prN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0a488a47-dcfa-42dd-b8d6-d1dad580cba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent attribute:\n",
      "--------------------------------------------------------------\n",
      "age:\t\t0.2602739726027397\n",
      "yredu:\t\t0.5333333333333333\n",
      "capgain:\t\t0.0\n",
      "caploss:\t\t0.0\n",
      "workhr:\t\t0.3979591836734694\n",
      "work:\t\t Private\n",
      "marriage:\t\t Married-civ-spouse\n",
      "occupation:\t\t Prof-specialty\n",
      "edu:\t\t HS-grad\n",
      "relationship:\t\t Husband\n",
      "sex:\t\t Male\n",
      "--------------------------------------------------------------\n",
      "Accuracy for baseline model for attribute 'marriage' is 46.68%\n"
     ]
    }
   ],
   "source": [
    "''' Answer: Test accuracy for attribute 'marriage' '''\n",
    "col_max = {}  # The baselinemodel prediction: get the most frequent value from each column\n",
    "\n",
    "print(\"Most frequent attribute:\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "for i in df_not_missing.columns: # choose the dataset df_not_missing so that the count of each column is the same\n",
    "    col_max[i] = df_not_missing[i].value_counts().idxmax() # find the most frequent one\n",
    "    print(\"{0}:\\t\\t{1}\".format(i, col_max[i]))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "accuracy = sum(df_not_missing['marriage'] == col_max['marriage']) / len(df_not_missing)\n",
    "print(\"Accuracy for baseline model for attribute 'marriage' is {}%\".format(round(accuracy * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "rhxWkFUeubbl",
    "outputId": "1bb47e6e-09bc-4dff-8fb9-d5c8d88608cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for baseline model for all attribute 'marriage' is 49.54%\n"
     ]
    }
   ],
   "source": [
    "''' Answer: Overall accuracy of all attributes'''\n",
    "acc = []\n",
    "for i in df_not_missing.columns:\n",
    "    accuracy = sum(df_not_missing[i] == col_max[i]) / len(df_not_missing)\n",
    "    acc.append(accuracy)\n",
    "    \n",
    "print(\"Average accuracy for baseline model for all attribute 'marriage' is {}%\".format(round(np.mean(acc) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlHu0wxh1prP"
   },
   "source": [
    "### Part (c) [1 pt]\n",
    "\n",
    "How does your test accuracy from part (a) compared to your basline test accuracy in part (b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "id": "1KQdwE_n1prQ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Average baseline accuracy is 49.54% from last part. \n",
    "# My model from part 4(a) has accuracy 63.78%, which is much better than the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfQPgu1Q1prS"
   },
   "source": [
    "### Part (d) [1 pt]\n",
    "\n",
    "Look at the first item in your test data. \n",
    "Do you think it is reasonable for a human\n",
    "to be able to guess this person's education level\n",
    "based on their other features? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "id": "3qbQ1vvT1prT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8f86d65e-356f-4c4a-e54d-62368bb10464"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>yredu</th>\n",
       "      <th>capgain</th>\n",
       "      <th>caploss</th>\n",
       "      <th>workhr</th>\n",
       "      <th>work_ Federal-gov</th>\n",
       "      <th>work_ Local-gov</th>\n",
       "      <th>work_ Private</th>\n",
       "      <th>work_ Self-emp-inc</th>\n",
       "      <th>work_ Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>edu_ Prof-school</th>\n",
       "      <th>edu_ Some-college</th>\n",
       "      <th>relationship_ Husband</th>\n",
       "      <th>relationship_ Not-in-family</th>\n",
       "      <th>relationship_ Other-relative</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  yredu  capgain  caploss   workhr  work_ Federal-gov  \\\n",
       "0  0.369863    0.8      0.0      0.0  0.44898                0.0   \n",
       "\n",
       "   work_ Local-gov  work_ Private  work_ Self-emp-inc  work_ Self-emp-not-inc  \\\n",
       "0              0.0            1.0                 0.0                     0.0   \n",
       "\n",
       "   ...  edu_ Prof-school  edu_ Some-college  relationship_ Husband  \\\n",
       "0  ...               0.0                0.0                    0.0   \n",
       "\n",
       "   relationship_ Not-in-family  relationship_ Other-relative  \\\n",
       "0                          1.0                           0.0   \n",
       "\n",
       "   relationship_ Own-child  relationship_ Unmarried  relationship_ Wife  \\\n",
       "0                      0.0                      0.0                 0.0   \n",
       "\n",
       "   sex_ Female  sex_ Male  \n",
       "0          1.0        0.0  \n",
       "\n",
       "[1 rows x 57 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=data.columns, data=[test_data[0]])\n",
    "\n",
    "# I would say that it's not reasonable for a human to guess a person's education level based on their other features.\n",
    "# There are not much correlations between education level and work, gender, marriage, capgain, etc.\n",
    "\n",
    "# Thus, my model only predict at 64% accuracy at last, not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_d5uuAY1prZ"
   },
   "source": [
    "### Part (e) [2 pt]\n",
    "\n",
    "What is your model's prediction of this person's education\n",
    "level, given their other features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BhWmDloVubbm",
    "outputId": "bf876564-c25f-4ab3-cd0f-ac1816dc0bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edu features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yredu</th>\n",
       "      <th>edu_ 10th</th>\n",
       "      <th>edu_ 11th</th>\n",
       "      <th>edu_ 12th</th>\n",
       "      <th>edu_ 1st-4th</th>\n",
       "      <th>edu_ 5th-6th</th>\n",
       "      <th>edu_ 7th-8th</th>\n",
       "      <th>edu_ 9th</th>\n",
       "      <th>edu_ Assoc-acdm</th>\n",
       "      <th>edu_ Assoc-voc</th>\n",
       "      <th>edu_ Bachelors</th>\n",
       "      <th>edu_ Doctorate</th>\n",
       "      <th>edu_ HS-grad</th>\n",
       "      <th>edu_ Masters</th>\n",
       "      <th>edu_ Preschool</th>\n",
       "      <th>edu_ Prof-school</th>\n",
       "      <th>edu_ Some-college</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yredu  edu_ 10th  edu_ 11th  edu_ 12th  edu_ 1st-4th  edu_ 5th-6th  \\\n",
       "0    0.8          0          0          0             0             0   \n",
       "\n",
       "   edu_ 7th-8th  edu_ 9th  edu_ Assoc-acdm  edu_ Assoc-voc  edu_ Bachelors  \\\n",
       "0             0         0                0               0               1   \n",
       "\n",
       "   edu_ Doctorate  edu_ HS-grad  edu_ Masters  edu_ Preschool  \\\n",
       "0               0             0             0               0   \n",
       "\n",
       "   edu_ Prof-school  edu_ Some-college  \n",
       "0                 0                  0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"edu features:\")\n",
    "edu_features = [value for value in data_features if 'edu' in value.lower()]\n",
    "data[edu_features][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ao1xJSZZubbm",
    "outputId": "9ab6eac7-6f05-4f2a-c352-5d75d222e43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bachelors\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data[0:1]) # only need the first one\n",
    "\n",
    "for item in test_loader:\n",
    "    int = item.detach().numpy()\n",
    "    out = encoder(zero_out_feature(item.clone(), \"edu\")).detach().numpy()\n",
    "    print(get_feature(out[0], \"edu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdLNA0ce1prd"
   },
   "source": [
    "### Part (f) [2 pt]\n",
    "\n",
    "What is the baseline model's prediction\n",
    "of this person's education level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "id": "TXgoM9qk1prd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "857ced84-9b33-4778-d856-766564885011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HS-grad\n"
     ]
    }
   ],
   "source": [
    "print(col_max['edu'])\n",
    "# Expected output of the baseline model would by HS-grad"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HwjDg1uM1pqe",
    "OEJ0Ci3l1pqh",
    "1_5ZZR_J1pqy",
    "WKk01pwx1pq_",
    "SxCTlXoV1prB",
    "h9xTwIf51prF",
    "UEe9yt6L1prM",
    "QlHu0wxh1prP",
    "DfQPgu1Q1prS",
    "p_d5uuAY1prZ",
    "fdLNA0ce1prd"
   ],
   "name": "Lab_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2360a000a0cfe5081460c17beecfe8caa8ffc0b971d04eec0b4ad72260c01bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
